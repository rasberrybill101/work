from fastapi import FastAPI, Query

app = FastAPI()

@app.get("/example")
async def example(param1: str = Query(...), param2: int = Query(...)):
    response = {
        "param1": param1,
        "param2": param2,
        "sum": param1 * param2
    }
    return response

===================================================================
Python fast api POST method that receives a JSON input of the form:
{ "chunk": "this is the input text",
  "history": "these are the previous chunks"
}

from fastapi import FastAPI
from pydantic import BaseModel

class Chunk(BaseModel):
    chunk: str
    history: str

app = FastAPI()

@app.post("/chunks/")
async def create_chunk(chunk: Chunk):
    return chunk


=================================================================
using python create a function "getLastK(k) to return the last k tokens of a space separated string

def getLastK(k, input_string):
    tokens = input_string.split()
    if len(tokens) < k:
        return input_string
    last_k_tokens = tokens[-k:]
    return " ".join(last_k_tokens)

======================== search for reason_level_1 within the conv. ==========
lmplement the following python function in a case-insensitive way:
Given a csv file with columns 
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
reason_level_2
plain_whisper
language_code

Ignore rows where:
column "complaint_disat" has the value "complaint"

column "reason_level_1" has any of the following values:
'*Case ID no Longer in CTR SQL Database'
'*Number did not match case'

create a dictionary of synonyms containing the following words and synonyms
FC, Financial Center
IVR, Automated System
payoff, pay off
payoffs, pay offs
mobile digital, app
write function getSynonym(word) that returns the synonym of a word. For example:
getSynonym('FC') will return 'Financial System'

for each row to be processed:
take the string in reason_level_1 column, replace non-alpha characters such as '(', ')', '/', '-' with a space
ensure that only single spaces are between words
trim the string

Find the first sentence in the plain_whisper column that contains the longest candidate word sequence that can be found in the reason_level_1 column (after all the non-alpha characters in the reason_level_1 column have been replaced with space characters as described previously). In addition, candidate word sequences can be created using synonyms from the dictionary given by "getSynonym()" defined above. For example:
Given reason_level_1 : "Call FC Transfers/Disconnects"
possible sequences are "call FC transfers disconnects", "call financial center transfers disconnects", "call transfers", ...
In this example, the longest sequence in reason_level_1 is "call financial center transfers disconnects". If this substring is found in the "plain_whisper" column, then the sentence that contains the candidate sequence will be selected as the "longest_candidate_sequence" string. Otherwise, search for any other possible identified sequence. If none can be found, then return "No Direct Sequence Found"
Notice that in the example above, 'FC' has a synonym and so it is included in the creation of word sequences.
To be clear, a sentence is defined as a sequence of words terminated by a period (the usual definition of a sentence).

To summarize, the sequence of tokens is initially identified from reason_level_1 tokens and then we search for them in plain_whisper column.
If a sequence is found in "plain_whisper" column, then the sentence that contains the sequence should be returned from the function that has been defined.
For example, in the string:
"it would seem simple. But we know that the quick brown fox jumps over the lazy dog quite quickly indeed. Today is when we find out."
if the sequence to search for is: "jumps over a", then the possible candidate sequences are:
"jumps over a", "jumps over", "over a", "jumps", "over", "a"
then the returned string will be:
"But we know that the quick brown fox jumps over the lazy dog quite quickly indeed"
This is basically the first sentence that contains the longest sub-sequence "jumps over"

complete the processing of all relevant rows before writing the following results to a csv file named "reason_level_1-phrases.txt" which has the following columns:
1) conv-id
2) complaint - if complaint_disat is 'complaint' then write 1 else write 0 in this column
2) reason_level_1
3) plain_whisper
4) sequence - longest candidate sequence from reason_level_1 that was found in the earliest sentence of plain_whisper
5) sentence - this is the sentence that contains the sequence
6) left character position of the returned sentence 
7) right character position of the returned sentence

columns (6) and (7) mark the starting and ending position of the sentence within the 'plain_whisper' text. This is done in order to easily highlight the sentence within the text of 'plain_whisper'.

Make the code modular by breaking it out into multiple smaller functions. It should be easy to port this code over to a jupyter notebook.

========================== sentences from sequencesimport pandas as pd
import csv
import re
from typing import List, Dict, Tuple

# Synonyms Dictionary
synonyms_dict = {
    'FC': 'Financial Center',
    'IVR': 'Automated System',
    'payoff': 'pay off',
    'payoffs': 'pay offs',
    'mobile digital': 'app'
}

ignored_reasons = ['*Case ID no Longer in CTR SQL Database', '*Number did not match case']

def getSynonym(word: str) -> str:
    return synonyms_dict.get(word, word)

def process_row(row: pd.Series) -> Dict[str, str]:
    complaint = 1 if row['complaint_disat'].lower() == 'complaint' else 0
    reason_level_1 = row['reason_level_1']
    plain_whisper = row['plain_whisper']

    # Replace non-alpha characters
    reason_level_1_clean = re.sub(r'[^a-zA-Z0-9 ]', ' ', reason_level_1).strip()

    # Create sequences
    sequences = generate_sequences(reason_level_1_clean, 2)

    # Find sequence and sentence
    sequence, sentence, left_char_pos, right_char_pos = find_sentence(plain_whisper, sequences)

    return {
        'conv_id': row['conv_id'],
        'complaint': complaint,
        'reason_level_1': reason_level_1,
        'plain_whisper': plain_whisper,
        'sequence': sequence,
        'sentence': sentence,
        'left_char_pos': left_char_pos,
        'right_char_pos': right_char_pos,
    }

def generate_sequences(reason_level_1_clean: str, min_length: int) -> List[str]:
    words = reason_level_1_clean.split(' ')
    words = [getSynonym(word) for word in words]
    sequences = [' '.join(words[i: j]) for i in range(len(words)) for j in range(i + 1, len(words) + 1) if len(words[i: j]) >= min_length]
    sequences.sort(key=len, reverse=True)
    return sequences

def find_sentence(plain_whisper: str, sequences: List[str]) -> Tuple[str, str, int, int]:
    sentences = plain_whisper.split('.')
    for sequence in sequences:
        for sentence in sentences:
            if sequence in sentence:
                start = plain_whisper.index(sentence)
                end = start + len(sentence)
                return sequence, sentence, start, end
    return 'No Direct Sequence Found', '', -1, -1

def process_csv(input_file: str, output_file: str):
    # Read CSV
    df = pd.read_csv(input_file)

    # Ignore rows
    df = df[df['complaint_disat'].str.lower() != 'complaint']
    df = df[~df['reason_level_1'].isin(ignored_reasons)]

    results = []

    # Process each row
    for index, row in df.iterrows():
        results.append(process_row(row))

    # Write to CSV
    result_df = pd.DataFrame(results)
    result_df.to_csv(output_file, index=False, quoting=csv.QUOTE_NONNUMERIC)

# Test the code
process_csv('your_input_file.csv', 'reason_level_1-phrases.txt')

========================== get sentences after introduction ===============
Write a function to get the next 3 (configurable as next_k) sentences after an introductory sentence (introductory_sentence) in a piece of text.
An introductory sentence has the following characteristics:
1. It occurs relatively early in the piece of text
2. It is very similar to one of the sentences in a list of introductory candidate sentences (introductory_candidates)

Once the introductory sentence is found, then this function should return the next k (next_k) sentences that immediately follow the introductory sentences.

Assume we have an initial list of introductory sentences:
how may I help you today
how can I help you today
what can I do for you
what can I do for you today
what may I do for you today
how can I assist you today
how can I be of assistance ... today
I will be able to assist ... today
how can I provide you ... today
how may I help you in today's call
how may I ... you today
how can I ... you today

The ellipsis ... means up to 4 words. For instance:
"how can I ... you today" can match any of the following:
"how can I be of service to you today"
"how can I provide service to you today"
"how can I help you today"
etc.

We will be looking for the last occurrence of the introductory sentence, and then fetch the next k sentences (where k is an argument to the extraction function).

The piece of text from which the next_k sentences will be obtained comes from a tab-separated file with the following columns (on the header line of the file):
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
reason_level_2
plain_whisper
language_code

The column 'plain_whisper' is where the introductory and next_k sentences are located.

Only consider rows where the value of the column 'complaint_disat' is 'Complaint'

Once all the introductory sentences have been identified, create a file named "possible_call_reason" which is a csv file with the following columns:
conv_id
complaint_disat
reason_level_1
possible_reason
plain_whisper

"possible_reason" is the next_k sentences
all other columns are from the input file

========================== introductory code
import pandas as pd
import re
import nltk

nltk.download('punkt')

introductory_candidates = [
    "how may I help you today",
    "how can I help you today",
    "what can I do for you",
    "what can I do for you today",
    "what may I do for you today",
    "how can I assist you today",
    "how can I be of assistance ... today",
    "I will be able to assist ... today",
    "how can I provide you ... today",
    "how may I help you in today's call",
    "how may I ... you today",
    "how can I ... you today",
]

def get_next_k_sentences(introductory_sentence, text, next_k):
    sentences = nltk.tokenize.sent_tokenize(text)
    for i in range(len(sentences)):
        if introductory_sentence in sentences[i]:
            return ' '.join(sentences[i+1:i+next_k+1])
    return ''

def find_introductory_sentence(text, introductory_candidates):
    sorted_candidates = sorted(introductory_candidates, key=len, reverse=True)  # sort from longest to shortest
    last_match = ''
    last_position = -1
    for candidate in sorted_candidates:
        if '...' in candidate:
            regex_pattern = candidate.replace('...', '(\w+\b\s*){0,4}')  # create regex pattern allowing up to 4 words
            for match in re.finditer(regex_pattern, text):  # find all matches
                if match.start() > last_position:  # if this match is later in the text
                    last_match = match.group()
                    last_position = match.start()
        elif candidate in text:
            position = text.rfind(candidate)  # find last occurrence of candidate
            if position > last_position:  # if this occurrence is later in the text
                last_match = candidate
                last_position = position
    return last_match

def process_data(file_name, next_k):
    df = pd.read_csv(file_name, sep='\t')
    df_complaint = df[df['complaint_disat'] == 'Complaint']
    for idx, row in df_complaint.iterrows():
        intro_sentence = find_introductory_sentence(row['plain_whisper'], introductory_candidates)
        if intro_sentence:
            possible_reason = get_next_k_sentences(intro_sentence, row['plain_whisper'], next_k)
            df_complaint.loc[idx, 'possible_reason'] = possible_reason
    df_complaint.to_csv('possible_call_reason.csv', index=False, columns=['conv_id', 'complaint_disat', 'reason_level_1', 'possible_reason', 'plain_whisper'])

process_data('file.tsv', 3)  # replace 'file.tsv' with your filename

---------

update the output to include these 2 additional columns to be written to the output file:
start_pos
end_pos

where start_pos is the character position of the beginning of next_k, and end_pos is the character position of the end of next_k.
Need this in order to highlight the next_k sentences in the text.

----------
import pandas as pd
import re
import nltk
from typing import List, Tuple

nltk.download('punkt')

introductory_candidates = [
    "how may I help you today",
    "how can I help you today",
    "what can I do for you",
    "what can I do for you today",
    "what may I do for you today",
    "how can I assist you today",
    "how can I be of assistance ... today",
    "I will be able to assist ... today",
    "how can I provide you ... today",
    "how may I help you in today's call",
    "how may I ... you today",
    "how can I ... you today",
]

def get_next_k_sentences(introductory_sentence: str, text: str, next_k: int) -> Tuple[str, int, int]:
    sentences = nltk.tokenize.sent_tokenize(text)
    for i in range(len(sentences)):
        if introductory_sentence in sentences[i]:
            next_k_text = ' '.join(sentences[i+1:i+next_k+1])
            start_pos = text.find(next_k_text)
            end_pos = start_pos + len(next_k_text)
            return next_k_text, start_pos, end_pos
    return '', -1, -1

def find_introductory_sentence(text: str, introductory_candidates: List[str]) -> str:
    sorted_candidates = sorted(introductory_candidates, key=len, reverse=True)  # sort from longest to shortest
    last_match = ''
    last_position = -1
    for candidate in sorted_candidates:
        if '...' in candidate:
            regex_pattern = candidate.replace('...', '(\w+\b\s*){0,4}')  # create regex pattern allowing up to 4 words
            for match in re.finditer(regex_pattern, text):  # find all matches
                if match.start() > last_position:  # if this match is later in the text
                    last_match = match.group()
                    last_position = match.start()
        elif candidate in text:
            position = text.rfind(candidate)  # find last occurrence of candidate
            if position > last_position:  # if this occurrence is later in the text
                last_match = candidate
                last_position = position
    return last_match

def process_data(file_name: str, next_k: int) -> None:
    df = pd.read_csv(file_name, sep='\t')
    df_complaint = df[df['complaint_disat'] == 'Complaint']
    for idx, row in df_complaint.iterrows():
        intro_sentence = find_introductory_sentence(row['plain_whisper'], introductory_candidates)
        if intro_sentence:
            possible_reason, start_pos, end_pos = get_next_k_sentences(intro_sentence, row['plain_whisper'], next_k)
            df_complaint.loc[idx, 'possible_reason'] = possible_reason
            df_complaint.loc[idx, 'start_pos'] = start_pos
            df_complaint.loc[idx, 'end_pos'] = end_pos
    df_complaint.to_csv('possible_call_reason.csv', index=False, 
                        columns=['conv_id', 'complaint_disat', 'reason_level_1', 'possible_reason', 'start_pos', 'end_pos', 'plain_whisper'])

process_data('metadata-labeled-conversations.txt', 5)

--------------- local copy of punkt
https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/tokenizers/punkt.zip
