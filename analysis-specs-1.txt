Analysis specs:
Given a tab separated file "raw_data.tsv" that has the following columns (specified in the header):
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
reason_level_2
plain_whisper
language_code

together with this code that extracts a datframe from the file:
#==============================================
# Input data
#==============================================
dir = '/chunk-complaints/categories/'
filename = 'metadata-labeled-conversations.txt'

import pandas as pd

#=======================================================================
# Read TSV file and return pandas df that excludes spanish language rows
#=======================================================================
def getTSVData(filename, sep='\t'):
    data = pd.read_csv(filename, sep=sep)
    return data

def rejectSpanish(data):
    data = data[data['plain_whisper'].apply(lambda x: 'gracias' not in str(x).lower())]
    return data

The dataframe obtained from rejectSpanish will be used as input to the following function
1. Given a define a function "rejectReasonLevels" that rejects any rows whose reason_level_1 contains tokens in a an input dataframe, and a list initialized to:
["CTRL SQL Database", "tbd"]
2. Given a dataframe (eg. the one obtained from rejectReasonLevels), write a function "showReasonDistribution" that will print a list of tab-separated lines for each reason_level_1, where each line contains:
unique value of reason_level_1
number of conv_id lines associated with reason_level_1
comma-separated list of individual tokens in the string value of reason_level_1

To print the individual tokens, treat non alphanumeric characters as spaces and separate the resulting string on spaces.

"showReasonDistribution" will also write each line to a file named "category_biz_labels.txt"

#=============================================
# Remove level-1 reasons not required
#=============================================
def rejectReasonLevels(data, levels_list=["CTR SQL Database", "tbd"]):
    # Convert the 'reason_level_1' column to strings
    data['reason_level_1'] = data['reason_level_1'].astype(str)
    
    for level in levels_list:
        # Case insensitive search
        data = data[~data['reason_level_1'].str.contains(level, case=False)]
    return data

#=============================================
# Show distribution of reason level 1
#=============================================
def showReasonDistribution(data, filename):
    reason_counts = data['reason_level_1'].value_counts()

    with open(filename, 'w') as f:
        for reason, count in reason_counts.items():
            reason_cleaned = re.sub('\W+', ' ', reason)
            tokens = reason_cleaned.split()
            print(f'{reason}\t{count}\t{",".join(tokens)}')
            f.write(f'{reason}\t{count}\t{",".join(tokens)}\n')


show a bar chart of reason_level_1 vs count

#=============================================
# Plot distribution of reason level 1
#=============================================
import matplotlib.pyplot as plt

def plotReasonDistribution(data, include_not_a_complaint=True):
    # Filter out 'Not a Complaint' rows if include_not_a_complaint is False
    if not include_not_a_complaint:
        data = data[data['reason_level_1'] != 'Not a Complaint']
        
    reason_counts = data['reason_level_1'].value_counts()

    # Sorting the counts in ascending order
    reason_counts = reason_counts.sort_values(ascending=True)

    # Increase the font size using plt.rc
    plt.rc('font', size=12) # controls default text size
    plt.rc('axes', titlesize=20) # controls axis title size
    plt.rc('axes', labelsize=20) # controls x and y labels size
    plt.rc('xtick', labelsize=12) # controls tick labels size
    plt.rc('ytick', labelsize=20) # controls tick labels size
    plt.rc('legend', fontsize=12) # controls legend size

    # Plotting
    plt.figure(figsize=(20,10))  # Adjust the size of the figure as per your needs
    reason_counts.plot(kind='barh')  # Changed to 'barh' for horizontal bars
    plt.ylabel('Reason Level 1')
    plt.xlabel('Count')
    if include_not_a_complaint:
        plt.title('Distribution of Reason Level 1')
        plt.savefig('plotWithComplaint.png', dpi=300)
    else:
        plt.title('Distribution of Reason Level 1 excluding "Not a Complaint"')
        plt.savefig('plotWithoutComplaint.png', dpi=300)

    # Add count annotations to each bar
    for index, value in enumerate(reason_counts):
        plt.text(value, index, str(value), fontsize=20)
        
    plt.tight_layout()
    plt.show()

--------------------------- show compact data
I want to make a compact "cheat sheet" that can be displayed on single printed page. It's also ok to take a screenshot and print that. The data I want to show will come from the data generated using this function: showReasonDistribution(data, filename)

#=============================================
# Tabulate reasons and counts
#=============================================
def tabulateDistribution(data, filename):
    reason_counts = data['reason_level_1'].value_counts()

    # Convert the series to a DataFrame
    df = reason_counts.reset_index()
    df.columns = ['reason_level_1', 'count']

    # Sort dataframe by count in descending order
    df.sort_values('count', ascending=False, inplace=True)

    # Write the DataFrame to a TSV file
    df.to_csv(filename, sep='\t', index=False)

------------------------ identify most common ngrams for a given category
Given a dataframe with the following columns:
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
reason_level_2
plain_whisper
language_code

Write a function 
ngramFrequencyInLevel1(data, ngramSize=2, topK=10, includedLevel1, excludedNgrams)

to identify the most commonly occurring ngrams in plain_whisper column for the rows where "complaint_disat" column has the value "Complaint", reason_level_1 value must be present in "includedLevel1", ngrams in the list "excludedNgrams" must be excluded. Do all ngram checking in a case-insensitive way.
Print "the ngram", count, reason_level_1, conv_id    for each of the most commonly occurring ngrams. The ngram should be printed in double quotes. Also append the printed results to the file "commonNgramsByReasonLevel.txt". Each row should be on a new line.

Use the defauts. Please put one ngram on each line for ease of extending the list:
excludedNgrams =["thank you", 
                "bank of", 
				"being a",
				"to be",
				"to do",
				"to go"
				"to you",
				"want to"
				"you for"
				]  Note the formatting of one entry per line makes it easy to add new lines!
includedLevel1 = ["Call Handling/Treatment",
				  "Card Issues (Credit/Debit/ATM)"
				 ] - again note one entry per line
topK means the topK ngrams that appear the largest number of times in all specific reason_level_1 column is present in "includedLevel1" and column "complaint_disat" has the value "Complaint".

ngram matching should be done at the token level, rather than at a character level. Therefore str.contains is not a valid way to check. By token is meant the individual words which can be separated by one or more spaces, and exclusive of periods, commas and other punctuation marks.

#=============================================
# Find ngram frequency by level for complaints
#=============================================
import pandas as pd
import re
from tqdm import tqdm
from datetime import datetime

def ngramFrequencyInLevel1(data, ngramSize=2, topK=10, includedLevel1=None, excludedNgrams=None, filename='commonNgramsByReasonLevel.txt'):
    if includedLevel1 is None:
        includedLevel1 = ["Call Handling/Treatment", "Card Issues (Credit/Debit/ATM)"]

    # If no excluded ngrams are provided, read from file  
    if excludedNgrams is None:
        with open('excludeNgrams.txt', 'r') as f:
            excludedNgrams = [line.strip().lower() for line in f]

    # Create a new DataFrame based on conditions
    new_data = data[(data['complaint_disat'] == 'Complaint') & (data['reason_level_1'].isin(includedLevel1))].copy()

    # Tokenize text into words and construct list of n-grams for each row
    new_data['plain_whisper_ngrams'] = new_data['plain_whisper'].apply(lambda x: [' '.join(re.findall(r'\b\w+\b', x.lower())[i:i+ngramSize]) for i in range(len(re.findall(r'\b\w+\b', x.lower()))-ngramSize+1)])

    # Remove excluded n-grams
    new_data['plain_whisper_ngrams'] = new_data['plain_whisper_ngrams'].apply(lambda ngrams: [ngram for ngram in ngrams if ngram not in excludedNgrams])

    # Count n-grams
    ngrams_series = pd.Series([ngram for ngrams_list in new_data['plain_whisper_ngrams'].tolist() for ngram in ngrams_list])
    top_ngrams = ngrams_series.value_counts().head(topK).reset_index().values.tolist()

    # Prepare output
    output = []

    for ngram, count in tqdm(top_ngrams, desc="Processing ngrams", unit="ngram"):
        matching_rows = new_data[new_data['plain_whisper_ngrams'].apply(lambda x: ngram in x)]
        for _, row in matching_rows.iterrows():
            output.append((ngram, count, row['reason_level_1'], row['conv_id']))
        
        # Print each distinct n-gram to console
        print(ngram)

    # Print and append to file
    with open(filename, 'a') as f:
        timestamp = datetime.now().strftime('%Y-%m-%d:%H-%M')
        f.write("#================== batch {} ================\n".format(timestamp))
        for item in output:
            f.write("ngram: \"{}\", count: {}, reason_level_1: \"{}\", conv_id: {}\n".format(*item))
            
    return output

modify what is written to file as follows:
"the ngram", sentence, count, reason_level_1, conv_id

Here "sentence" means the sentence that contains the ngram. For example for ngram "thank you" then the sentence could be something like "I would like to say thank you for doing that". Sentence has the usual meaning of a block of text terminating with a period.

----- updated to include sentence containing the ngram
import re
import pandas as pd
from tqdm import tqdm
from datetime import datetime

def extract_sentences(text):
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', text)
    return sentences

def extract_ngrams(sentence, ngramSize):
    tokens = re.findall(r'\b\w+\b', sentence.lower())
    ngrams = [' '.join(tokens[i:i+ngramSize]) for i in range(len(tokens)-ngramSize+1)]
    return ngrams

def ngramFrequencyInLevel1(data, ngramSize=2, topK=10, includedLevel1=None, excludedNgrams=None, filename='commonNgramsByReasonLevel.txt'):
    if includedLevel1 is None:
        includedLevel1 = ["Call Handling/Treatment", "Card Issues (Credit/Debit/ATM)"]

    if excludedNgrams is None:
        with open('excludeNgrams.txt', 'r') as f:
            excludedNgrams = [line.strip().lower() for line in f]

    new_data = data[(data['complaint_disat'] == 'Complaint') & (data['reason_level_1'].isin(includedLevel1))].copy()

    sentences_ngrams = []
    ngrams_sentence_map = {}
    output_rows = set()

    for index, row in new_data.iterrows():
        sentences = extract_sentences(row['plain_whisper'])
        for sentence in sentences:
            ngrams = extract_ngrams(sentence, ngramSize)
            ngrams = [ngram for ngram in ngrams if ngram not in excludedNgrams]
            sentences_ngrams.extend(ngrams)
            for ngram in ngrams:
                if ngram not in ngrams_sentence_map:
                    ngrams_sentence_map[ngram] = {(sentence, index)}
                else:
                    ngrams_sentence_map[ngram].add((sentence, index))

    ngrams_series = pd.Series(sentences_ngrams)
    top_ngrams = ngrams_series.value_counts().head(topK).reset_index().values.tolist()

    output = []

    for ngram, count in tqdm(top_ngrams, desc="Processing ngrams", unit="ngram"):
        if ngram in ngrams_sentence_map:
            for sentence, row_index in ngrams_sentence_map[ngram]:
                if row_index not in output_rows:
                    row = new_data.loc[row_index]
                    output.append((ngram, sentence, count, row['reason_level_1'], row['conv_id']))
                    output_rows.add(row_index)
        print(ngram)

    with open(filename, 'a') as f:
        timestamp = datetime.now().strftime('%Y-%m-%d:%H-%M')
        f.write("#================== batch {} ================\n".format(timestamp))
        for item in output:
            f.write("ngram: \"{}\", sentence: \"{}\", count: {}, reason_level_1: \"{}\", conv_id: \"{}\"\n".format(*item))

    return output

===================== additional ...
can you update the code so that only sentences that contain the ngrams in the file "allowedNgrams.txt" are included in the output. "allowedNgrams.txt" is a text file with one ngram per line, where each ngram is a string consisting of one or more tokens

def ngramFrequencyInLevel1(data, ngramSize=2, topK=10, includedLevel1=None, excludedNgrams=None, filename='commonNgramsByReasonLevel.txt'):
    if includedLevel1 is None:
        includedLevel1 = ["Call Handling/Treatment", "Card Issues (Credit/Debit/ATM)"]

    if excludedNgrams is None:
        with open('excludeNgrams.txt', 'r') as f:
            excludedNgrams = [line.strip().lower() for line in f]

    # Load allowed ngrams, skipping lines that start with '#'
    with open('allowedNgrams.txt', 'r') as f:
        allowedNgrams = [line.strip().lower() for line in f if not line.startswith('#')]

    new_data = data[(data['complaint_disat'] == 'Complaint') & (data['reason_level_1'].isin(includedLevel1))].copy()

    sentences_ngrams = []
    ngrams_sentence_map = {}
    output_rows = set()

    for index, row in new_data.iterrows():
        sentences = extract_sentences(row['plain_whisper'])
        for sentence in sentences:
            ngrams = extract_ngrams(sentence, ngramSize)
            ngrams = [ngram for ngram in ngrams if ngram not in excludedNgrams and ngram in allowedNgrams]
            sentences_ngrams.extend(ngrams)
            for ngram in ngrams:
                if ngram not in ngrams_sentence_map:
                    ngrams_sentence_map[ngram] = {(sentence, index)}
                else:
                    ngrams_sentence_map[ngram].add((sentence, index))

    ngrams_series = pd.Series(sentences_ngrams)
    top_ngrams = ngrams_series.value_counts().head(topK).reset_index().values.tolist()

    output = []

    for ngram, count in tqdm(top_ngrams, desc="Processing ngrams", unit="ngram"):
        if ngram in ngrams_sentence_map:
            for sentence, row_index in ngrams_sentence_map[ngram]:
                if row_index not in output_rows:
                    row = new_data.loc[row_index]
                    output.append((ngram, sentence, count, row['reason_level_1'], row['conv_id']))
                    output_rows.add(row_index)
        print(ngram)

    with open(filename, 'a') as f:
        timestamp = datetime.now().strftime('%Y-%m-%d:%H-%M')
        f.write("#================== batch {} ================\n".format(timestamp))
        for item in output:
            f.write("\"{}\", \"{}\", {}, \"{}\", {}\n".format(*item))

    return output

-------------------------------- unique ngrams for reason_level_1
Given an input dataframe with the following columns:
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
plain_whisper

Write function getUniqueNgrams(data, ngramSize, toFilename) that returns a list comprising:
ngram, reason_level_1, conv_id, complaint_disat

Here are the steps to be taken by the function getUniqueNgrams 
1) find the ngrams from 'plain_whisper' column of each row in the dataframe where 'complaint_disat' column has the value 'Complaint'. The ngrams can be found using the following pre-defined functions:

def extract_sentences(text):
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', text)
    return sentences

def extract_ngrams(sentence, ngramSize):
    tokens = re.findall(r'\b\w+\b', sentence.lower())
    ngrams = [' '.join(tokens[i:i+ngramSize]) for i in range(len(tokens)-ngramSize+1)]
    return ngrams

The ngrams obtained from this step belong to the corresponding 'reason_level_1', 'complaint_disat' pair
	
2) From the list of ngrams obtained in step 1, find the ngrams that are NOT present in any other 'reason_level_1', 'complaint_disat' pair.

These are the unique ngrams for 'reason_level_1'

For example, given the following:
if ngram1, ngram2, ngram3, ngram8, ngram12 are the ngrams that belong to reason_level_1 == 'rl1', and complaint_disat == 'Complaint'
and ngram2, ngram4, ngram8, ngram9 are the ngrams that belong to reason_level_1 == 'rl2', and complaint_disat == 'Complaint'

then the unique ngrams for rl1 are ngram1, ngram3, ngram12
and the unique ngrams for rl2 are ngram4, ngram9

(assuming that these ngrams do not appear in any other rows where complaint_disat == 'Complaint')

The outputs should be printed out:
print(ngram, reason_level_1, conv_id, complaint_disat)
and also written to the filename "toFilename" which has default value of "uniqueNgrams.txt" as a comma separated string where ngram is enclosed in quotes

----------------------
You have misunderstood what is required. Specifically what uniqueness means. Here is a restatement of the steps to be taken by the function getUniqueNgrams 

1) Find the ngrams from 'plain_whisper' column of each row in the dataframe where 'complaint_disat' column has the value 'Complaint', using the pre-defined functions extract_sentences and extract_ngrams

2) From the list of ngrams obtained in step 1, find the ngrams in each reason_level_1 that are NOT present in any other 'reason_level_1', 'complaint_disat' pair.

For example, given if all ngrams from rows where reason_level_1 is 'rl1' are:
ngram1, ngram2, ngram3, ngram8, ngram12 (reason_level_1 == 'rl1', and complaint_disat == 'Complaint')
and all ngrams from rows where reason_level_1 is 'rl2'
ngram2, ngram4, ngram8, ngram9 (reason_level_1 == 'rl2', and complaint_disat == 'Complaint')

then the set of unique ngrams for rl1 are ngram1, ngram3, ngram12
and the set of unique ngrams for rl2 are ngram4, ngram9

What makes an ngram a member of the unique set of ngrams for a given reason_level_1 (for which the value of 'complaint_disat' column is 'Complaint') is the fact that any ngram in the set is not present in the set of unique ngrams that belong to other reason_level_1 ngram sets.

------- update
update the output to include the sentence where each unique ngram was found. Therefore, each output line should look like this:
ngram, reason_level_1, sentence, conv_id, complaint_disat
and the corresponding print statement should look like:
print(ngram, reason_level_1, sentence, conv_id, complaint_disat)

#=============================================
# Find unique ngrams for each reason_level_1
#=============================================
import re
import pandas as pd
from collections import defaultdict
from tqdm import tqdm

def getUniqueNgrams(data, ngramSize=1, toFilename='uniqueNgrams.txt'):
    ngrams_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(set)))

    # Step 1: Extract ngrams and associate them with their corresponding 'reason_level_1', sentence, and conv_id
    for _, row in tqdm(data.iterrows(), desc="Extracting ngrams"):
        if row.complaint_disat:  # we only want to consider rows where complaint_disat is not None
            sentences = extract_sentences(row.plain_whisper)
            for sentence in sentences:
                ngrams = extract_ngrams(sentence, ngramSize)
                for ngram in ngrams:
                    ngrams_dict[row.reason_level_1][ngram][row.conv_id].add(sentence)

    # Step 2: Find unique ngrams for each 'reason_level_1'
    unique_ngrams = {}
    for rl1, ngrams_rl1 in tqdm(ngrams_dict.items(), desc="Processing ngrams"):
        unique_ngrams_rl1 = ngrams_rl1.copy()
        for rl2, ngrams_rl2 in ngrams_dict.items():
            if rl1 != rl2:  # we don't want to compare the set with itself
                unique_ngrams_rl1 = {ngram: conv_ids for ngram, conv_ids in unique_ngrams_rl1.items() if ngram not in ngrams_rl2}
        unique_ngrams[rl1] = unique_ngrams_rl1

    # Step 3: Write to file and print to console
    with open(toFilename, 'w', encoding='utf-8') as f:
        with open('ngram_reason.txt', 'w', encoding='utf-8') as f2:
            for rl1, unique_ngrams_rl1 in unique_ngrams.items():
                for ngram, conv_ids in unique_ngrams_rl1.items():
                    for conv_id, sentences in conv_ids.items():
                        for sentence in sentences:
                            complaint_disat = data.loc[data['conv_id'] == conv_id, 'complaint_disat'].values[0]  # get complaint_disat value for the current conv_id
                            f2.write(f'"{ngram}",{rl1},"{sentence}"\n')  # write to ngram_reason.txt
                            line = f'"{ngram}",{rl1},"{sentence}",{conv_id},"{complaint_disat}"\n'
                            f.write(line)
                            print(ngram, rl1, sentence)

    return unique_ngrams


    
------------------- additional convenience requirement

additionally, and for convenience, for each ngram only write the following columns to another file named "ngram_reason.txt"
ngram, reason_level_1, sentence. Use this code as the basis for doing this:
	
----------------------------------------------- only include certain reason_level_1 values
Given a dataframe with the following columns:
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
plain_whisper

Write a function "onlyIncludeReasons(includedLevel1)

that returns a dataframe that only includes rows where reason_level_1 is in this list:
includedLevel1 = ["Call Handling/Treatment", "Card Issues (Credit/Debit/ATM)", "Employee Misconduct Allegation"]

If includedLevel1 is None, then the values above should be used

#=============================================
# Filter data to only include specific reason_level_1 values
#=============================================
def onlyIncludeReasons(data, includedLevel1=None):
    if includedLevel1 is None:
        includedLevel1 = ["Call Handling/Treatment", "Card Issues (Credit/Debit/ATM)", "Employee Misconduct Allegation"]
    
    return data[data.reason_level_1.isin(includedLevel1)]
