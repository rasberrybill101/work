Analysis specs:
Given a tab separated file "raw_data.tsv" that has the following columns (specified in the header):
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
reason_level_2
plain_whisper
language_code

together with this code that extracts a datframe from the file:
#==============================================
# Input data
#==============================================
dir = '/chunk-complaints/categories/'
filename = 'metadata-labeled-conversations.txt'

import pandas as pd

#=======================================================================
# Read TSV file and return pandas df that excludes spanish language rows
#=======================================================================
def getTSVData(filename, sep='\t'):
    data = pd.read_csv(filename, sep=sep)
    return data

def rejectSpanish(data):
    data = data[data['plain_whisper'].apply(lambda x: 'gracias' not in str(x).lower())]
    return data

The dataframe obtained from rejectSpanish will be used as input to the following function
1. Given a define a function "rejectReasonLevels" that rejects any rows whose reason_level_1 contains tokens in a an input dataframe, and a list initialized to:
["CTRL SQL Database", "tbd"]
2. Given a dataframe (eg. the one obtained from rejectReasonLevels), write a function "showReasonDistribution" that will print a list of tab-separated lines for each reason_level_1, where each line contains:
unique value of reason_level_1
number of conv_id lines associated with reason_level_1
comma-separated list of individual tokens in the string value of reason_level_1

To print the individual tokens, treat non alphanumeric characters as spaces and separate the resulting string on spaces.

"showReasonDistribution" will also write each line to a file named "category_biz_labels.txt"

#=============================================
# Remove level-1 reasons not required
#=============================================
def rejectReasonLevels(data, levels_list=["CTR SQL Database", "tbd"]):
    # Convert the 'reason_level_1' column to strings
    data['reason_level_1'] = data['reason_level_1'].astype(str)
    
    for level in levels_list:
        # Case insensitive search
        data = data[~data['reason_level_1'].str.contains(level, case=False)]
    return data

#=============================================
# Show distribution of reason level 1
#=============================================
def showReasonDistribution(data, filename):
    reason_counts = data['reason_level_1'].value_counts()

    with open(filename, 'w') as f:
        for reason, count in reason_counts.items():
            reason_cleaned = re.sub('\W+', ' ', reason)
            tokens = reason_cleaned.split()
            print(f'{reason}\t{count}\t{",".join(tokens)}')
            f.write(f'{reason}\t{count}\t{",".join(tokens)}\n')


show a bar chart of reason_level_1 vs count

#=============================================
# Plot distribution of reason level 1
#=============================================
import matplotlib.pyplot as plt

def plotReasonDistribution(data, include_not_a_complaint=True):
    # Filter out 'Not a Complaint' rows if include_not_a_complaint is False
    if not include_not_a_complaint:
        data = data[data['reason_level_1'] != 'Not a Complaint']
        
    reason_counts = data['reason_level_1'].value_counts()

    # Sorting the counts in ascending order
    reason_counts = reason_counts.sort_values(ascending=True)

    # Increase the font size using plt.rc
    plt.rc('font', size=12) # controls default text size
    plt.rc('axes', titlesize=20) # controls axis title size
    plt.rc('axes', labelsize=20) # controls x and y labels size
    plt.rc('xtick', labelsize=12) # controls tick labels size
    plt.rc('ytick', labelsize=20) # controls tick labels size
    plt.rc('legend', fontsize=12) # controls legend size

    # Plotting
    plt.figure(figsize=(20,10))  # Adjust the size of the figure as per your needs
    reason_counts.plot(kind='barh')  # Changed to 'barh' for horizontal bars
    plt.ylabel('Reason Level 1')
    plt.xlabel('Count')
    if include_not_a_complaint:
        plt.title('Distribution of Reason Level 1')
        plt.savefig('plotWithComplaint.png', dpi=300)
    else:
        plt.title('Distribution of Reason Level 1 excluding "Not a Complaint"')
        plt.savefig('plotWithoutComplaint.png', dpi=300)

    # Add count annotations to each bar
    for index, value in enumerate(reason_counts):
        plt.text(value, index, str(value), fontsize=20)
        
    plt.tight_layout()
    plt.show()

--------------------------- show compact data
I want to make a compact "cheat sheet" that can be displayed on single printed page. It's also ok to take a screenshot and print that. The data I want to show will come from the data generated using this function: showReasonDistribution(data, filename)

#=============================================
# Tabulate reasons and counts
#=============================================
def tabulateDistribution(data, filename):
    reason_counts = data['reason_level_1'].value_counts()

    # Convert the series to a DataFrame
    df = reason_counts.reset_index()
    df.columns = ['reason_level_1', 'count']

    # Sort dataframe by count in descending order
    df.sort_values('count', ascending=False, inplace=True)

    # Write the DataFrame to a TSV file
    df.to_csv(filename, sep='\t', index=False)

------------------------ identify most common ngrams for a given category
Given a dataframe with the following columns:
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
reason_level_2
plain_whisper
language_code

Write a function 
ngramFrequencyInLevel1(data, ngramSize=2, topK=10, includedLevel1, excludedNgrams)

to identify the most commonly occurring ngrams in plain_whisper column for the rows where "complaint_disat" column has the value "Complaint", reason_level_1 value must be present in "includedLevel1", ngrams in the list "excludedNgrams" must be excluded. Do all ngram checking in a case-insensitive way.
Print "the ngram", count, reason_level_1, conv_id    for each of the most commonly occurring ngrams. The ngram should be printed in double quotes. Also append the printed results to the file "commonNgramsByReasonLevel.txt". Each row should be on a new line.

Use the defauts. Please put one ngram on each line for ease of extending the list:
excludedNgrams =["thank you", 
                "bank of", 
				"being a",
				"to be",
				"to do",
				"to go"
				"to you",
				"want to"
				"you for"
				]  Note the formatting of one entry per line makes it easy to add new lines!
includedLevel1 = ["Call Handling/Treatment",
				  "Card Issues (Credit/Debit/ATM)"
				 ] - again note one entry per line
topK means the topK ngrams that appear the largest number of times in all specific reason_level_1 column is present in "includedLevel1" and column "complaint_disat" has the value "Complaint".

ngram matching should be done at the token level, rather than at a character level. Therefore str.contains is not a valid way to check. By token is meant the individual words which can be separated by one or more spaces, and exclusive of periods, commas and other punctuation marks.

#=============================================
# Find ngram frequency by level for complaints
#=============================================
import pandas as pd
import re
from tqdm import tqdm
from datetime import datetime

def ngramFrequencyInLevel1(data, ngramSize=2, topK=10, includedLevel1=None, excludedNgrams=None, filename='commonNgramsByReasonLevel.txt'):
    if includedLevel1 is None:
        includedLevel1 = ["Call Handling/Treatment", "Card Issues (Credit/Debit/ATM)"]

    # If no excluded ngrams are provided, read from file  
    if excludedNgrams is None:
        with open('excludeNgrams.txt', 'r') as f:
            excludedNgrams = [line.strip().lower() for line in f]

    # Create a new DataFrame based on conditions
    new_data = data[(data['complaint_disat'] == 'Complaint') & (data['reason_level_1'].isin(includedLevel1))].copy()

    # Tokenize text into words and construct list of n-grams for each row
    new_data['plain_whisper_ngrams'] = new_data['plain_whisper'].apply(lambda x: [' '.join(re.findall(r'\b\w+\b', x.lower())[i:i+ngramSize]) for i in range(len(re.findall(r'\b\w+\b', x.lower()))-ngramSize+1)])

    # Remove excluded n-grams
    new_data['plain_whisper_ngrams'] = new_data['plain_whisper_ngrams'].apply(lambda ngrams: [ngram for ngram in ngrams if ngram not in excludedNgrams])

    # Count n-grams
    ngrams_series = pd.Series([ngram for ngrams_list in new_data['plain_whisper_ngrams'].tolist() for ngram in ngrams_list])
    top_ngrams = ngrams_series.value_counts().head(topK).reset_index().values.tolist()

    # Prepare output
    output = []

    for ngram, count in tqdm(top_ngrams, desc="Processing ngrams", unit="ngram"):
        matching_rows = new_data[new_data['plain_whisper_ngrams'].apply(lambda x: ngram in x)]
        for _, row in matching_rows.iterrows():
            output.append((ngram, count, row['reason_level_1'], row['conv_id']))
        
        # Print each distinct n-gram to console
        print(ngram)

    # Print and append to file
    with open(filename, 'a') as f:
        timestamp = datetime.now().strftime('%Y-%m-%d:%H-%M')
        f.write("#================== batch {} ================\n".format(timestamp))
        for item in output:
            f.write("ngram: \"{}\", count: {}, reason_level_1: \"{}\", conv_id: {}\n".format(*item))
            
    return output

modify what is written to file as follows:
"the ngram", sentence, count, reason_level_1, conv_id

Here "sentence" means the sentence that contains the ngram. For example for ngram "thank you" then the sentence could be something like "I would like to say thank you for doing that". Sentence has the usual meaning of a block of text terminating with a period.

----- updated to include sentence containing the ngram
import re
import pandas as pd
from tqdm import tqdm
from datetime import datetime

def extract_sentences(text):
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', text)
    return sentences

def extract_ngrams(sentence, ngramSize):
    tokens = re.findall(r'\b\w+\b', sentence.lower())
    ngrams = [' '.join(tokens[i:i+ngramSize]) for i in range(len(tokens)-ngramSize+1)]
    return ngrams

def ngramFrequencyInLevel1(data, ngramSize=2, topK=10, includedLevel1=None, excludedNgrams=None, filename='commonNgramsByReasonLevel.txt'):
    if includedLevel1 is None:
        includedLevel1 = ["Call Handling/Treatment", "Card Issues (Credit/Debit/ATM)"]

    if excludedNgrams is None:
        with open('excludeNgrams.txt', 'r') as f:
            excludedNgrams = [line.strip().lower() for line in f]

    new_data = data[(data['complaint_disat'] == 'Complaint') & (data['reason_level_1'].isin(includedLevel1))].copy()

    sentences_ngrams = []
    ngrams_sentence_map = {}
    output_rows = set()

    for index, row in new_data.iterrows():
        sentences = extract_sentences(row['plain_whisper'])
        for sentence in sentences:
            ngrams = extract_ngrams(sentence, ngramSize)
            ngrams = [ngram for ngram in ngrams if ngram not in excludedNgrams]
            sentences_ngrams.extend(ngrams)
            for ngram in ngrams:
                if ngram not in ngrams_sentence_map:
                    ngrams_sentence_map[ngram] = {(sentence, index)}
                else:
                    ngrams_sentence_map[ngram].add((sentence, index))

    ngrams_series = pd.Series(sentences_ngrams)
    top_ngrams = ngrams_series.value_counts().head(topK).reset_index().values.tolist()

    output = []

    for ngram, count in tqdm(top_ngrams, desc="Processing ngrams", unit="ngram"):
        if ngram in ngrams_sentence_map:
            for sentence, row_index in ngrams_sentence_map[ngram]:
                if row_index not in output_rows:
                    row = new_data.loc[row_index]
                    output.append((ngram, sentence, count, row['reason_level_1'], row['conv_id']))
                    output_rows.add(row_index)
        print(ngram)

    with open(filename, 'a') as f:
        timestamp = datetime.now().strftime('%Y-%m-%d:%H-%M')
        f.write("#================== batch {} ================\n".format(timestamp))
        for item in output:
            f.write("ngram: \"{}\", sentence: \"{}\", count: {}, reason_level_1: \"{}\", conv_id: \"{}\"\n".format(*item))

    return output

===================== additional ...
can you update the code so that only sentences that contain the ngrams in the file "allowedNgrams.txt" are included in the output. "allowedNgrams.txt" is a text file with one ngram per line, where each ngram is a string consisting of one or more tokens

def ngramFrequencyInLevel1(data, ngramSize=2, topK=10, includedLevel1=None, excludedNgrams=None, filename='commonNgramsByReasonLevel.txt'):
    if includedLevel1 is None:
        includedLevel1 = ["Call Handling/Treatment", "Card Issues (Credit/Debit/ATM)"]

    if excludedNgrams is None:
        with open('excludeNgrams.txt', 'r') as f:
            excludedNgrams = [line.strip().lower() for line in f]

    # Load allowed ngrams, skipping lines that start with '#'
    with open('allowedNgrams.txt', 'r') as f:
        allowedNgrams = [line.strip().lower() for line in f if not line.startswith('#')]

    new_data = data[(data['complaint_disat'] == 'Complaint') & (data['reason_level_1'].isin(includedLevel1))].copy()

    sentences_ngrams = []
    ngrams_sentence_map = {}
    output_rows = set()

    for index, row in new_data.iterrows():
        sentences = extract_sentences(row['plain_whisper'])
        for sentence in sentences:
            ngrams = extract_ngrams(sentence, ngramSize)
            ngrams = [ngram for ngram in ngrams if ngram not in excludedNgrams and ngram in allowedNgrams]
            sentences_ngrams.extend(ngrams)
            for ngram in ngrams:
                if ngram not in ngrams_sentence_map:
                    ngrams_sentence_map[ngram] = {(sentence, index)}
                else:
                    ngrams_sentence_map[ngram].add((sentence, index))

    ngrams_series = pd.Series(sentences_ngrams)
    top_ngrams = ngrams_series.value_counts().head(topK).reset_index().values.tolist()

    output = []

    for ngram, count in tqdm(top_ngrams, desc="Processing ngrams", unit="ngram"):
        if ngram in ngrams_sentence_map:
            for sentence, row_index in ngrams_sentence_map[ngram]:
                if row_index not in output_rows:
                    row = new_data.loc[row_index]
                    output.append((ngram, sentence, count, row['reason_level_1'], row['conv_id']))
                    output_rows.add(row_index)
        print(ngram)

    with open(filename, 'a') as f:
        timestamp = datetime.now().strftime('%Y-%m-%d:%H-%M')
        f.write("#================== batch {} ================\n".format(timestamp))
        for item in output:
            f.write("\"{}\", \"{}\", {}, \"{}\", {}\n".format(*item))

    return output

