Analysis specs:
Given a tab separated file "raw_data.tsv" that has the following columns (specified in the header):
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
reason_level_2
plain_whisper
language_code

together with this code that extracts a datframe from the file:
#==============================================
# Input data
#==============================================
dir = '/chunk-complaints/categories/'
filename = 'metadata-labeled-conversations.txt'

import pandas as pd

#=======================================================================
# Read TSV file and return pandas df that excludes spanish language rows
#=======================================================================
def getTSVData(filename, sep='\t'):
    data = pd.read_csv(filename, sep=sep)
    return data

def rejectSpanish(data):
    data = data[data['plain_whisper'].apply(lambda x: 'gracias' not in str(x).lower())]
    return data

The dataframe obtained from rejectSpanish will be used as input to the following function
1. Given a define a function "rejectReasonLevels" that rejects any rows whose reason_level_1 contains tokens in a an input dataframe, and a list initialized to:
["CTRL SQL Database", "tbd"]
2. Given a dataframe (eg. the one obtained from rejectReasonLevels), write a function "showReasonDistribution" that will print a list of tab-separated lines for each reason_level_1, where each line contains:
unique value of reason_level_1
number of conv_id lines associated with reason_level_1
comma-separated list of individual tokens in the string value of reason_level_1

To print the individual tokens, treat non alphanumeric characters as spaces and separate the resulting string on spaces.

"showReasonDistribution" will also write each line to a file named "category_biz_labels.txt"

#=============================================
# Remove level-1 reasons not required
#=============================================
def rejectReasonLevels(data, levels_list=["CTR SQL Database", "tbd"]):
    # Convert the 'reason_level_1' column to strings
    data['reason_level_1'] = data['reason_level_1'].astype(str)
    
    for level in levels_list:
        # Case insensitive search
        data = data[~data['reason_level_1'].str.contains(level, case=False)]
    return data

#=============================================
# Show distribution of reason level 1
#=============================================
def showReasonDistribution(data, filename):
    reason_counts = data['reason_level_1'].value_counts()

    with open(filename, 'w') as f:
        for reason, count in reason_counts.items():
            reason_cleaned = re.sub('\W+', ' ', reason)
            tokens = reason_cleaned.split()
            print(f'{reason}\t{count}\t{",".join(tokens)}')
            f.write(f'{reason}\t{count}\t{",".join(tokens)}\n')


show a bar chart of reason_level_1 vs count

#=============================================
# Plot distribution of reason level 1
#=============================================
import matplotlib.pyplot as plt

def plotReasonDistribution(data, include_not_a_complaint=True):
    # Filter out 'Not a Complaint' rows if include_not_a_complaint is False
    if not include_not_a_complaint:
        data = data[data['reason_level_1'] != 'Not a Complaint']
        
    reason_counts = data['reason_level_1'].value_counts()

    # Sorting the counts in ascending order
    reason_counts = reason_counts.sort_values(ascending=True)

    # Increase the font size using plt.rc
    plt.rc('font', size=12) # controls default text size
    plt.rc('axes', titlesize=20) # controls axis title size
    plt.rc('axes', labelsize=20) # controls x and y labels size
    plt.rc('xtick', labelsize=12) # controls tick labels size
    plt.rc('ytick', labelsize=20) # controls tick labels size
    plt.rc('legend', fontsize=12) # controls legend size

    # Plotting
    plt.figure(figsize=(20,10))  # Adjust the size of the figure as per your needs
    reason_counts.plot(kind='barh')  # Changed to 'barh' for horizontal bars
    plt.ylabel('Reason Level 1')
    plt.xlabel('Count')
    if include_not_a_complaint:
        plt.title('Distribution of Reason Level 1')
        plt.savefig('plotWithComplaint.png', dpi=300)
    else:
        plt.title('Distribution of Reason Level 1 excluding "Not a Complaint"')
        plt.savefig('plotWithoutComplaint.png', dpi=300)

    # Add count annotations to each bar
    for index, value in enumerate(reason_counts):
        plt.text(value, index, str(value), fontsize=20)
        
    plt.tight_layout()
    plt.show()

--------------------------- show compact data
I want to make a compact "cheat sheet" that can be displayed on single printed page. It's also ok to take a screenshot and print that. The data I want to show will come from the data generated using this function: showReasonDistribution(data, filename)

#=============================================
# Tabulate reasons and counts
#=============================================
def tabulateDistribution(data, filename):
    reason_counts = data['reason_level_1'].value_counts()

    # Convert the series to a DataFrame
    df = reason_counts.reset_index()
    df.columns = ['reason_level_1', 'count']

    # Sort dataframe by count in descending order
    df.sort_values('count', ascending=False, inplace=True)

    # Write the DataFrame to a TSV file
    df.to_csv(filename, sep='\t', index=False)

------------------------ identify most common ngrams for a given category
Given a dataframe with the following columns:
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
reason_level_2
plain_whisper
language_code

Write a function 
ngramFrequencyInLevel1(data, ngramSize=2, topK=10, includedLevel1, excludedNgrams)

to identify the most commonly occurring ngrams in plain_whisper column for the rows where "complaint_disat" column has the value "Complaint", reason_level_1 value must be present in "includedLevel1", ngrams in the list "excludedNgrams" must be excluded. Do all ngram checking in a case-insensitive way.
Print "the ngram", count, reason_level_1, conv_id    for each of the most commonly occurring ngrams. The ngram should be printed in double quotes. Also append the printed results to the file "commonNgramsByReasonLevel.txt". Each row should be on a new line.

Use the defauts. Please put one ngram on each line for ease of extending the list:
excludedNgrams =["thank you", 
                "bank of", 
				"being a",
				"to be",
				"to do",
				"to go"
				"to you",
				"want to"
				"you for"
				]  Note the formatting of one entry per line makes it easy to add new lines!
includedLevel1 = ["Call Handling/Treatment",
				  "Card Issues (Credit/Debit/ATM)"
				 ] - again note one entry per line
topK means the topK ngrams that appear the largest number of times in all specific reason_level_1 column is present in "includedLevel1" and column "complaint_disat" has the value "Complaint".

ngram matching should be done at the token level, rather than at a character level. Therefore str.contains is not a valid way to check. By token is meant the individual words which can be separated by one or more spaces, and exclusive of periods, commas and other punctuation marks.

#=============================================
# Find ngram frequency by level for complaints
#=============================================
import pandas as pd
import re
from sklearn.feature_extraction.text import CountVectorizer

def ngramFrequencyInLevel1(data, ngramSize=2, topK=10, includedLevel1=None, excludedNgrams=None):
    if includedLevel1 is None:
        includedLevel1 = ["Call Handling/Treatment", "Card Issues (Credit/Debit/ATM)"]
    
    if excludedNgrams is None:
        excludedNgrams = ["thank you", "bank of", "being a", "to be", "to do", "to go", "to you", "want to", "you for"]
    
    # Convert excludedNgrams to lower case as we're doing case-insensitive matching
    excludedNgrams = [x.lower() for x in excludedNgrams]

    # Filter DataFrame based on conditions
    data = data[(data['complaint_disat'] == 'Complaint') & (data['reason_level_1'].isin(includedLevel1))]

    # Initialize CountVectorizer
    vec = CountVectorizer(ngram_range=(ngramSize, ngramSize), stop_words=excludedNgrams, lowercase=True)

    # Fit and transform on 'plain_whisper' column
    word_count_vec = vec.fit_transform(data['plain_whisper'])

    # Summarize the word counts
    sum_words = word_count_vec.sum(axis=0)
    
    # Get words and corresponding counts
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)
    
    # Select top K n-grams
    top_ngrams = words_freq[:topK]

    # Tokenize text into words and construct list of n-grams for each row
    data['plain_whisper_ngrams'] = data['plain_whisper'].apply(lambda x: [' '.join(re.findall(r'\b\w+\b', x.lower())[i:i+ngramSize]) for i in range(len(re.findall(r'\b\w+\b', x.lower()))-ngramSize+1)])

    # Prepare output
    output = []
    for ngram, count in top_ngrams:
        matching_rows = data[data['plain_whisper_ngrams'].apply(lambda x: ngram in x)]
        for _, row in matching_rows.iterrows():
            output.append((ngram, count, row['reason_level_1'], row['conv_id']))

    # Print and append to file
    with open('commonNgramsByReasonLevel.txt', 'a') as f:
        for item in output:
            f.write("ngram: \"{}\", count: {}, reason_level_1: \"{}\", conv_id: {}\n".format(*item))
            
    return output
