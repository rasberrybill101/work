=========================== START OF SPECIFICATION 
Find sentences that contain keywords or key phrases for a specific category

Write a function "assembleCategoryPhraseDictionary(inputFilename: str = "categoryPhraseData.txt") -> Dict[str, List[str]]:" that returns a dictionary

The input data is from a file named by default "categoryPhraseData.txt" that has the following format and rules:
any line that starts with "#" is a comment and should be ignored when processing "categoryPhraseData.txt"
:category 1 - a category name ("category 1") immediately preceded by a leading colon ":"
value1
value2
...
valuek  - the kth value of category1
# start of category 2
:category 2
val. 1
val. 2
...
val. m - the mth value of category 2. Again all values are strings

...

# start of another category
:another category
val 1
val 2
...
last value

"assembleCategoryPhraseDictionary(...)" will skip all comments and blank lines until it reaches a category name - lines that begin with a colon (:) character followed by a string that can contain any character including spaces. This string is the category name.
Each line after the category name represents a phrase that has to be associated with that category.
If a category name is repeated (found in another line), then all the phrases found for all instances of the category are assembled into a single large list that will be associated with the category name.
The output from "assembleCategoryPhraseDictionary" is a dictionary of category phrases. For the example above, this dictionary  is as follows:

{
  "category 1" : ["value1", "value2", ... "valuek"],
  "category 2" : ["val. 1", "val. 2", ... "val. m"],
  ...
  "another category" : ["val 1", "val 2", ... "last value"]
}

Write a function "serializeCategoryPhraseDictionary(dict: Dict[str, list[str]], delimiter: str = '\t', outputFilename: str = "serializedCategoryPhraseDict.tsv") that could be called from "assembleCategoryPhraseDictionary(...) that serializes the dictionary to a 2 column tab-delimited file as follows:

category, phrase is the header line
category 1, value1 (delimiter is specified in the function)
category 1, value2
...
category 1, valuek
category 2, val. 1
category 2, val. 2
...
category 2, val. m
another category, val 1
another category, val 2
...
another category, last value

Write a function "deserializeCategoryPhraseDictionary(filename: str, delimiter: str = '\t') -> Dict[str, List[str]] that returns a category phrase dictionary from a serialized file such as "serializedCategoryPhraseDict.tsv".

#%%
#================================================================
# assemble category phrases from raw phrase file of category names and likely phrases
#================================================================
import os
from typing import Dict, List

def assembleCategoryPhraseDictionary(inputFilename: str = "categoryPhraseData.txt") -> Dict[str, List[str]]:
    cat_phrase_dict = {}
    current_cat = None

    with open(inputFilename, 'r') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):  # skip blank lines and comments
                continue
            if line.startswith(':'):  # new category
                current_cat = line[1:]
                if current_cat not in cat_phrase_dict:
                    cat_phrase_dict[current_cat] = []
            else:  # a phrase in the current category
                if current_cat is not None:
                    cat_phrase_dict[current_cat].append(line)
        
    return cat_phrase_dict
	
def serializeCategoryPhraseDictionary(dict: Dict[str, List[str]], delimiter: str = '\t', outputFilename: str = "serializedCategoryPhraseDict.tsv"):
    with open(outputFilename, 'w') as f:
        f.write(f'category{delimiter}phrase\n')  # write the header line
        for cat, phrases in dict.items():
            for phrase in phrases:
                f.write(f'{cat}{delimiter}{phrase}\n')  # write each phrase on a new line

#%%
#================================================================
# invoke assembly of category phrases
#================================================================
dir = "/chunk-complaint/category/"
catPhraseDataPath = dir + "category_phrase_data.txt"
cat_phrase_dict = assembleCategoryPhraseDictionary(catPhraseDataPath)
outputFilename = "serialized_category_phrase_dict.tsv"
serializeCategoryPhraseDictionary(dict=cat_phrase_dict, delimiter='\t', outputFilename=outputFilename)

#%%
#================================================================
# Deserialize category phrase dictionary file
#================================================================
import csv
from typing import Dict, List

def deserializeCategoryPhraseDictionary(filename: str = "serializedCategoryPhraseDict.tsv", delimiter: str = '\t') -> Dict[str, List[str]]:
    cat_phrase_dict = {}

    with open(filename, 'r') as f:
        reader = csv.reader(f, delimiter=delimiter)
        next(reader)  # Skip the header
        for row in reader:
            cat, phrase = row
            if cat in cat_phrase_dict:
                cat_phrase_dict[cat].append(phrase)
            else:
                cat_phrase_dict[cat] = [phrase]
                
    return cat_phrase_dict

--------------------------------------------------------------------------------------------------------------------

Given that a string can be decomposed into sentences based on the following definition:
def extract_sentences(text):
	sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.(?!\d)|\?|!)\s', text)
    return sentences
	
Write a function "sentencesForPhrase(text: str, phrase: str, n: int = 2) -> List[str]"
that returns up to n (zero, one, or more) sentences where "text" contains "phrase". The list will be empty if the phrase is not found in text. For generality, the check for whether "phrase" is in "text" should be separately defined in a function "phraseBelongsToText(text: str, phrase: str) -> bool" because different algorithms may be tried in order to check if a phrase can be related to a sentence in the text.

#%%
#================================================================
# Given some text, return a list of sentences associated with a phrase
#================================================================

import re
import string
from typing import List

def phraseBelongsToText(text: str, phrase: str) -> bool:
    # remove punctuation
    translator = str.maketrans('', '', string.punctuation)
    text = text.translate(translator)
    phrase = phrase.translate(translator)

    # convert to lower case
    text = text.lower()
    phrase = phrase.lower()

    # check for the phrase in the text
    return phrase in text

def extract_sentences(text: str) -> List[str]:
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.(?!\d)|\?|!)\s', text)
    return sentences

def sentencesForPhrase(text: str, phrase: str, n: int = 2) -> List[str]:
    sentences = extract_sentences(text)
    relevant_sentences = [sentence for sentence in sentences if phraseBelongsToText(sentence, phrase)]
    return relevant_sentences[:n]
	
--------------------------------------------------------------------------------------------------------------------
Modification of specs
Given the previously defined function
def sentencesForPhrase(text: str, phrase: str, n: int = 2) -> List[str]:

Modify the data returned from the function so that the position of each sentence is also returned. For example if the 1st, 3rd, and 6th sentences were returned, then the function should return the these positions together with each sentence at the position. Currently the function only returns the list of sentences, but does not return their position in the original list of sentences used to select sentences for phrase.

#================================================================
# REPLACEMENT/UPDATE
#================================================================
def sentencesForPhrase(text: str, phrase: str, n: int = 2) -> List[Tuple[int, str]]:
    sentences = extract_sentences(text)
    relevant_sentences = [(i, sentence) for i, sentence in enumerate(sentences) if phraseBelongsToText(sentence, phrase)]
    return relevant_sentences[:n]

--------------------------------------------------------------------------------------------------------------------

Write a function "getTrainingSentences(inputFile: str = "train_data_v3_metadata.txt", delimiter: str = '\t', categoryPhraseDictionary: Dict[category: str, phrases: List[str]]) -> Dict[str, List[str]]:", where:
inputFile has columns (with delimiter specified in the function) :
conv_id
label
call_duration
reason_level_1
reason_level_2
plain_whisper

For each row in inputFile, a list of phrases will be obtained from categoryPhraseDictionary where the category key for the dictionary is the value of "reason_level_1" column. This list of phrases for the category is then used to assemble a list of sentences using "sentencesForPhrase(text, phrase, 3)" for each phrase, where:
"text" is the value of "plain_whisper" column. The result of the function call will be a dictionary of categories, each with a list of sentences:
{
  "category 1": ["sentence 1", "sentence 2", ....],
  ...
  "category k": ["first sentence", "second sentence", ...]
}

Write a function "serializeCategorySentences(outputFilename: str = "categoryTrainingSentences.tsv", delimiter: str = '\t', Dict[str, List[str]])" such that it writes the category sentences to a named output file in 3 columns as follows (given the example dictionary above):
category, training_sentence, conv_id is the header line
category 1, "sentence 1", id1
category 1, "sentence 2", id2
...
category k, "first sentence", id3
category k, "second sentence", id4
...

#%%
#================================================================
# Assemble training sentences from training metadata
#================================================================
import csv
from typing import Dict, List

def getTrainingSentences(inputFile: str = "train_data_v3_metadata.txt", delimiter: str = '\t',  sentenceCount: int = 3, categoryPhraseDictionary: Dict[str, List[str]]) -> Dict[str, List[tuple]]:
    training_sentences = {}

    with open(inputFile, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f, delimiter=delimiter)
        for row in reader:
            category = row['reason_level_1']
            text = row['plain_whisper']
            conv_id = row['conv_id']

            if category in categoryPhraseDictionary:
                for phrase in categoryPhraseDictionary[category]:
                    sentences = sentencesForPhrase(text, phrase, sentenceCount)
                    sentences_with_id = [(conv_id, sentence) for sentence in sentences]
                    if category not in training_sentences:
                        training_sentences[category] = sentences_with_id
                    else:
                        training_sentences[category].extend(sentences_with_id)
                        
    return training_sentences

#================================================================
# Serialize the training data to file
#================================================================
def serializeCategorySentences(outputFilename: str = "categoryTrainingSentences.tsv", delimiter: str = '\t', data: Dict[str, List[tuple]]):
    with open(outputFilename, 'w') as f:
        f.write(f'category{delimiter}training_sentence{delimiter}conv_id\n')  # header line
        for category, sentences in data.items():
            for conv_id, sentence in sentences:
                f.write(f'{category}{delimiter}"{sentence}"{delimiter}{conv_id}\n')

#%%
#================================================================
# Invoke assembly and serialization of category training data sentences
#================================================================
dir = "/chunk-complaint/category/"
raw_training_filename = dir + "train_data_v3_metadata.txt"
training_sentences = getTrainingSentences(inputFile=raw_training_filename, delimiter='\t', sentenceCount=3, categoryPhraseDictionary=cat_phrase_dict)
outFilename = dir + "category_training_sentences.tsv"
serializeCategorySentences(outputFilename=outFilename, delimiter='\t', training_sentences)

--------------------------------------------------------------------------------------------------------------------
Based on this modification of sentencesForPhrase, modify:
getTrainingSentences - to accomodate the change in sentencesForPhrase
serializeCategorySentences - to accomodate change and to now return these columns (in the stated order):
category
position - new column - position of the sentence obtained from the change to sentencesForPhrase
sentence
conv_id

#================================================================
# REPLACEMENT/UPDATE
#================================================================
def getTrainingSentences(inputFile: str = "train_data_v3_metadata.txt", delimiter: str = '\t', categoryPhraseDictionary: Dict[str, List[str]], sentenceCount: int = 3) -> Dict[str, List[Tuple[int, str, str]]]:
    training_sentences = {}

    with open(inputFile, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f, delimiter=delimiter)
        for row in reader:
            category = row['reason_level_1']
            text = row['plain_whisper']
            conv_id = row['conv_id']

            if category in categoryPhraseDictionary:
                for phrase in categoryPhraseDictionary[category]:
                    indexed_sentences = sentencesForPhrase(text, phrase, sentenceCount)
                    sentences_with_id = [(pos, sentence, conv_id) for pos, sentence in indexed_sentences]
                    if category not in training_sentences:
                        training_sentences[category] = sentences_with_id
                    else:
                        training_sentences[category].extend(sentences_with_id)
                        
    return training_sentences

def serializeCategorySentences(outputFilename: str = "categoryTrainingSentences.tsv", delimiter: str = '\t', data: Dict[str, List[Tuple[int, str, str]]]):
    with open(outputFilename, 'w') as f:
        writer = csv.writer(f, delimiter='\t')
        writer.writerow(['category', 'position', 'training_sentence', 'conv_id'])  # write the header line
        for category, sentences in data.items():
            for position, sentence, conv_id in sentences:
                writer.writerow([category, position, sentence, conv_id])  # write each sentence on a new line

--------------------------------------------------------------------------------------------------------------------
Given two files "train_data_v3_metadata.txt" (file1), and "category_training_sentences.tsv" (file2).
Where file1 has the following colums:
conv_id
label
call_duration
reason_level_1
reason_level_2
plain_whisper

and file2 has the following columns:
category (same as reason_level_1 of file1)
sentence
conv_id (same as conv_id of file1)

Write a function "missingTrainingData(file1: str, file2: str, n=10) -> List[tuple]" that has the following columns:
category
sentences
conv_id

where each column in the list of rows returned by "missingTrainingData" is created as follows:
category is the same as the file2 category
conv_id only conv_ids present in file1 but not in file2 should be present in the returned rows
sentences is the first N sentences of plain_whisper column of file 1
and N is the arg n in the function.

Please use the following definition of sentences to fetch the first N sentences from plain_whisper column of file1:
def extract_sentences(text):
	sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.(?!\d)|\?|!)\s', text)
    return sentences

Write a function "serializeMissingTraining(outputFilename)" to write the results of "missingTrainingData" to the file "missing_training_data.txt" with columns in the following order:
category, conv_id, sentences

#%%
#================================================================
# Identify and return first N sentences from missing training IDs
#================================================================
import csv
import re
from typing import List, Tuple

def extract_sentences(text):
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.(?!\d)|\?|!)\s', text)
    return sentences

def missingTrainingData(file1: str, file2: str, n=10) -> List[Tuple[str, str, List[str]]]:
    missing_data = []

    # Read categories from file2
    with open(file2, 'r') as f:
        reader = csv.DictReader(f, delimiter='\t')
        file2_categories = {row['category'] for row in reader}

    # Read rows from file1 and check category
    with open(file1, 'r') as f:
        reader = csv.DictReader(f, delimiter='\t')
        for row in reader:
            category = row['reason_level_1']
            if category not in file2_categories:
                continue
            conv_id = row['conv_id']
            sentences = extract_sentences(row['plain_whisper'])[:n]
            missing_data.append((category, conv_id, sentences))

    return missing_data

#%%
#================================================================
# Serialize the missing training topN
#================================================================
def serializeMissingTraining(outputFilename: str, data: List[Tuple[str, str, List[str]]], delimiter: str = '\t'):
    with open(outputFilename, 'w', encoding='utf-8') as f:
        f.write(f'category{delimiter}conv_id{delimiter}sentences\n')  # write the header
        for category, conv_id, sentences in data:
            f.write(f'{category}{delimiter}{conv_id}{delimiter}"{" ".join(sentences)}"\n')

#%%
dir = "/chunk-complaint/category/"
data = missingTrainingData(dir + "train_data_v3_metadata.txt", dir + "category_training_sentences.tsv")
serializeMissingTraining(dir + "missing_training_data.txt", data)