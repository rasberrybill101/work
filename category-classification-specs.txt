Spec 1: reformat the initial signal annotation
----------------------------------------------
Given a tab separated file1 named "initial-category-signals.txt", create a new file2 named "all-category-signals.txt"
file1 has the following columns
category - a string
sentence - a string
signal - a string

column "text" is a string that has the following format:
this is a sentence that has two signals, here is the {sig-1:first signal found} and {sig-1:here is the second signal}

column "signal" is a comma separated list of strings that contains the same number of strings as there are occurrences of "sig-1:" in the "sentence" column. For example:
signal-1, the-second-signal

The goal is to write each "sentence" string to the output file2, by replacing each occurrence of "sig-1" in "sentence" with the corresponding positioned string in "signal". Thus, given the examples of "sentence" and "signal" columns above, then the following line would be written to file2:

this is an example that has two signals, here is the {signal-1:first signal found} and {the-second-signal:here is the second signal}

Thus file2 will have the same number of lines as file1, but will contain each string in file1 "sentence" where "sig-1" is replaced by the corresponding signal value in the "signal" column.

Spec 2: create a signals dictionary
-----------------------------------
A signals dictionary is defined as a dictionary of key-value pairs, where each key is a string and each value is a list of strings (phrases). For example:
signalsDictionary = {
    "signal-1": ["Phrase 1", "Another phrase 1"],
    "signal-2": ["Phrase 2"],
    "signal-3": ["Yet another phrase", "And one more"],
    "signal-4": ["Last phrase"]
}

Given a list of strings from a text file named "all-category-signals.txt", where each string is on a single line that contains one or more signals. Here is a sample file with 3 lines:
this is a sentence that has two signals, here is the {signal-1:first signal found} and {signal-2:here is the second signal}
for this sentence {signal-1:there is another occurrence of a signal} and {signal-3:another one}
this one has {signal-4:a different signal} and then {signal-3:yet another signal}

Write a python function "getCategorySignals(filename="all-category-signals.txt") that will return the signalsDictionary for this file:

{
    "signal-1": ["first signal found", "there is another occurrence of a signal"],
    "signal-2": ["here is the second signal"],
    "signal-3": ["another one", "yet another signal"],
    "signal-4": ["a different signal"]
}

Spec 3: use the signalsDictionary to encode sentence and conversation vectors
-----------------------------------------------------------------------------
The goal is to convert a conversation into a vector representation using a function called "encodeConversation". A conversation is a list of sentences extracted using the function "extract_sentences" when given the entire text of a conversation as follows:
def extract_sentences(text):
	sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.(?!\d)|\?|!)\s', text)
    return sentences

The vector represention of a conversation is built up from the vector representation of the individual sentences in the conversation as follows: 

There is a dictionary of key-value pairs ("signalsDictionary") whose keys represent signals (each signal is a string), and whose values are lists of strings. Here is an example of the "signalsDictionary":
signalsDictionary = {
    "s1": ["Phrase 1", "Another phrase 1"],
    "s2": ["Phrase 2"],
    "s3": ["Yet another phrase", "And one more"],
    "s4": ["Last phrase"]
}
which has 4 signals ("s1" through "s4"), and for each key a value which is a list of strings (phrases that comprise one or more space-separated tokens).
Using the signalsDictionary, a vector of the same length as the number of keys in the signalsDictionary will be used to encode the individual sentences of a conversation as vectors in the following way:
A vector of length N (the number of signals) representing the conversation is a list of N real numbers initialized to zero:
	convVector = [0, 0, 0, 0, ... 0] - there are N zeros in convVector
After initialization, convVector is  built up using the vectors of the individual sentences of the conversation.
For each sentence in the conversation, a vector:
	sentenceVector is initialized to [0, 0, 0, 0, ... 0] - (of size N - same size as convVector)
The sentence is checked for the occurrence of one or more phrases associated with each key. If a phrase is found (sentence contains the entire phrase), then the value in sentenceVector that corresponds to the position of the key is incremented by 1. For example, if phrase matches are found at positions 1, 2, and 4 (keys "s1", "s2", and "s4") then the sentenceVector will be:
[1, 1, 0, 1]
Thus, the values in each sentence of the conversation is used to update the convVector simply by updating convVector by adding each sentenceVector to it.
To keep the implementation flexible so that we can easily change how the vector is updated, then write a function "encodeSentence" that takes a sentence and the signalsDictionary, then returns a single sentence vector.
The function "encodeSentence(sentence, signalsDictionary)" can then be called from within "encodeConversation(convText, signalsDictionary, normalize=True)" where the vector of the conversation is obtained by summing the vectors of all individual sentences in the conversation. If normalize is True, then the values in the conversation vector are scaled such that the max. value of any entry in the vector is 1. For example the vector:
[8, 2, 0, 4] would become [1, 0.25, 0, 0.5]
Note that the reason for having a separate "encodeSentence" function is because want to have a flexible way of defining how a vector is obtained from a sentence given the phrase dictionary.

Spec 4 : Update the defintion of encodeSentence
-----------------------------------------------
In the function "encodeSentence", Instead of just trying to do an exact match on one of the phrases of a signal:

            if phrase in sentence:  # If the phrase is in the sentence
                sentenceVector[i] += 1  # Increment the corresponding position in the sentence vector

Use the longest common sequence of tokens between the phrase and the sentence. Where "findLongestCommonSequence(text, phrase)" is defined as follows:
1) express both the text and the phrase as a list of tokens. Treat the text and phrase as space-separated tokens, and ignore any punctuation marks in the list of tokens and convert everything to lowercase. Therefore:
text: "I'm going for a walk, since it is a fine day."
tokens: i'm going for a walk since it is a fine day  (all lowercase and all punctuations removed)
punctuations are: commas, periods, exclamation marks, question marks, semi-colons, colons

2) find the number of tokens in the phrase that are present in the sentence (in the same order)
3) the tokens in the sentence do not have to be consecutive. they just need to be present in the same order as in the phrase.
For example, if:
sentence is: "I am going for a walk since it is a really fine day today"
phrase is: "walk to the road since it has a Fine box"

The longest common sequence is "walk since a fine". The length is 4 (count of tokens in the longest common sequence)
However, if the phrase is: "Today is a Fine but cloudy day, and i will walk to the road"
then the longest common sequence of tokens is "fine day". The length is 2.

The length (number of tokens) of the longest common sequence is then used to set the value of the sentenceVector.


------------------------------ implementation
#==================================================
# create conversation vector from sentence vectors
#==================================================
import numpy as np
import re

def extract_sentences(text): # Split on period, ?, and !
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.(?!\d)|\?|!)\s', text)
    return sentences  # list of sentences

def encodeSentence(sentence, signalsDictionary):
    sentenceVector = np.zeros(len(signalsDictionary))  # Initialize the sentence vector
    for i, key in enumerate(signalsDictionary.keys()):  # Go through each signal in the dictionary
        for phrase in signalsDictionary[key]:  # Go through each phrase associated with the signal
            if phrase in sentence:  # If the phrase is in the sentence
                sentenceVector[i] += 1  # Increment the corresponding position in the sentence vector
    return sentenceVector


def encodeConversation(convText, signalsDictionary, normalize=True):
    convVector = np.zeros(len(signalsDictionary))  # Initialize the conversation vector
    sentences = extract_sentences(convText)  # Extract sentences from the conversation
    for sentence in sentences:  # Go through each sentence
        sentenceVector = encodeSentence(sentence, signalsDictionary)  # Get the vector for the sentence
        convVector += sentenceVector  # Add the sentence vector to the conversation vector
    
    if normalize:  # If we want to normalize the conversation vector
        max_value = np.max(convVector)  # Find the maximum value in the conversation vector
        if max_value > 0:  # Avoid division by zero
            convVector /= max_value  # Scale the conversation vector so that the maximum value is 1

    return convVector.tolist()  # Convert the numpy array back to a Python list before returning  

----------------------- invocation
signalsDictionary = {
    "s1": ["Phrase 1", "Another phrase 1"],
    "s2": ["Phrase 2"],
    "s3": ["Yet another phrase", "And one more"],
    "s4": ["Last phrase"]
}

convText = """
This conversation contains Phrase 1, but also Another phrase 1.
Sometimes a conversation will contain Yet another phrase. 
Last phrase is the end. 
"""

vector = encodeConversation(convText, signalsDictionary)
print(vector)

----------------------- extract phrase from sentence
I want to be able to create the signal dictionary from annotated sentences. Sentences are annotated by enclosing certain tokens within curly braces as follows:
 "category 1", "this sentences contains {an annotated phrase}:sig-1"
 "category 2", "here are two annotated phrases showing {the first one}:sig-1 and {also a second one}:sig-2"
 "category 3", "and this is {another annotation}:sig-1 {on this sentence}:sig-3"
 "category 4", "and this is another {annotation on this}:sig-4 sentence"
 
 Given a file ("categorySignals.txt") comprising lines of annotated sentences as above, then write function "createsignalsDictionary(filename='categorySignals.txt')" that returns a signalsDictionary such as (based on the sample sentences above):
 signalsDictionary = {
    "sig-1": ["an annotated phrase", "the first one", "another annotation"],
    "sig-2": ["also a second one"],
    "sig-3": ["on this sentence"],
    "sig-4": ["annotation on this"]
}

Write another function that saves the dictionary ("signalsDictionary.txt") to file in json format.
Write another function that reads the signalsDictionary file and returns a corresponding signalsDictionary.

Keep implementation modular, and show sample invocation of each of the functions.

#==================================================
# create signal dictionary from annotated sentences
#==================================================
import re
import json

def createsignalsDictionary(filename='categorySignals.txt'):
    # Dictionary to hold the signals
    signalsDictionary = defaultdict(list)

    with open(filename, 'r') as f:
        for line in f.readlines():
            # Extract all annotations from the line
            annotations = re.findall(r'{(.*?)}:(\w+)', line)
            
            # Add each phrase to its corresponding signal in the dictionary
            for phrase, signal in annotations:
                signalsDictionary[signal].append(phrase)

    return signalsDictionary
    
def savesignalsDictionary(signalsDictionary, filename='signalsDictionary.txt'):
    with open(filename, 'w') as f:
        json.dump(signalsDictionary, f)

def loadsignalsDictionary(filename='signalsDictionary.txt'):
    with open(filename, 'r') as f:
        signalsDictionary = json.load(f)
    return signalsDictionary

--------------------------------- linear regression model training

Given conversation vectors created using the following function:
def encodeConversation(convText, signalsDictionary, normalize=True):
    convVector = np.zeros(len(signalsDictionary))  # Initialize the conversation vector
    sentences = extract_sentences(convText)  # Extract sentences from the conversation
    for sentence in sentences:  # Go through each sentence
        sentenceVector = encodeSentence(sentence, signalsDictionary)  # Get the vector for the sentence
        convVector += sentenceVector  # Add the sentence vector to the conversation vector
    
    if normalize:  # If we want to normalize the conversation vector
        max_value = np.max(convVector)  # Find the maximum value in the conversation vector
        if max_value > 0:  # Avoid division by zero
            convVector /= max_value  # Scale the conversation vector so that the maximum value is 1

    return convVector.tolist()  # Convert the numpy array back to a Python list before returning 
        
        
and with the following training data file "categoryTrainingData.txt" (where the first line is a header line):
category_label, expectedConversationVector
"category 1", [0.1, 0.3, 0.9, 1, 0, 0, 0.8]
"category 2", [0.2, 0.7, 0, 0.1, 0, 0, 0.5]  
...

where "category X" represents the expected category, and the expectedConversationVector is a comma-separated list of the elements of the conversationVector for the given category.

and a test data file "categoryTestData.txt" (where the first line is a header):
conversation, predictedConversationVector
"this is conversation 1", [0.3, 0.2, 1, 0, 0.3, 0.6, 0.4]

Write the function that will use the training and test data to create and train a logistic regression model to predict category from a conversation vector. Modularize the code and show an example of how the logistic regression model can be used to make a prediction. From the soft max output, if none of the categories reach a threshold value, then return the category "No Category" which means that there is insufficient confidence to predict a category, given the input vector.

Write the linear regression model to the file "categoryModel" with a suitable file extension. Also provide a function to load the model from the file.

------------ additional data:
You said that the code makes the following assumptions:    
The vectors in the files are properly preprocessed.
The labels in the training data are integer labels starting from 0.      
However, I pointed out that the labels and vectors are in this form:   
category_label, expectedConversationVector
"category 1", [0.1, 0.3, 0.9, 1, 0, 0, 0.8]
"category 2", [0.2, 0.7, 0, 0.1, 0, 0, 0.5] 

So you will need to make required modifications to use label strings (for example encode the labels as integers, etc.), and note the way that the vectors have been provided. Please show entire code taking these facts into account. 


#==================================================
# train and use a logistic regression model for category prediction
#==================================================
import pandas as pd
import numpy as np
import json
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import pickle

def load_data(filename, is_train):
    # Load the data from a CSV file
    df = pd.read_csv(filename)
    
    # The 'expectedConversationVector' column contains lists
    X = np.array(df['expectedConversationVector'].tolist())
    
    if is_train:
        # Convert the labels to integers
        le = LabelEncoder()
        y = le.fit_transform(df['category_label'])
        return X, y, le
    else:
        return X, df['category_label']

# Train the model
def train_model(X, y):
    model = LogisticRegression(max_iter=1000)
    model.fit(X, y)
    return model

# Save the model and label encoder
def save_model(model, le, model_filename='categoryModel.pkl', le_filename='labelEncoder.pkl'):
    with open(model_filename, 'wb') as f:
        pickle.dump(model, f)
    with open(le_filename, 'wb') as f:
        pickle.dump(le, f)

# Load the model and label encoder
def load_model(model_filename='categoryModel.pkl', le_filename='labelEncoder.pkl'):
    with open(model_filename, 'rb') as f:
        model = pickle.load(f)
    with open(le_filename, 'rb') as f:
        le = pickle.load(f)
    return model, le

# Make a prediction
def predict_category(input_vector, model, le):
    prediction = model.predict_proba([input_vector])
    if prediction.max() < 0.5:  # if confidence is lower than 0.5, return 'No Category'
        return 'No Category'
    else:
        # Otherwise, return the category with highest probability
        return le.inverse_transform([prediction.argmax()])[0]

# Evaluate the model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy, precision, recall, f1

# Main function to run the code
def main():
    # Load and process the training data
    X_train, y_train, le = load_data('categoryTrainingData.txt', is_train=True)
    # Train the model
    model = train_model(X_train, y_train)
    # Save the model and label encoder
    save_model(model, le)

    # Load the test data
    X_test, y_test = load_data('categoryTestData.txt', is_train=False)
    # Evaluate the model
    accuracy, precision, recall, f1 = evaluate_model(model, X_test, y_test)
    print(f'Accuracy: {accuracy}\nPrecision: {precision}\nRecall: {recall}\nF1 Score: {f1}')

    # Load the model and label encoder
    model, le = load_model()
    # Use the loaded model to make a prediction
    print(predict_category(X_test[0], model, le))  # Predict the category of the first test example

if __name__ == "__main__":
    main()


====================================================
Create a function "embedSignalInSentence(ngram, sentence)" that searches for the occurrence of the ngram within the sentence and replaces the ngram with "{sig-1:<ngram>}". For example, given:
"ngram" is "I tried to"
"sentence" is "Yesterday, when I tried to use the card"
then the function will return:
"Yesterday, when {sig-1:I tried to} use the card"


def embedSignalInSentence(ngram, sentence):
    return sentence.replace(ngram, "{{sig-1:{}}}".format(ngram))

# Example usage
ngram = "I tried to"
sentence = "Yesterday, when I tried to use the card"
result = embedSignalInSentence(ngram, sentence)
print(result)
