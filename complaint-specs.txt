Given a tab separated file "raw_data.tsv" that has the following columns (specified in the header):
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
reason_level_2
plain_whisper
language_code

Each row represents a conversation that identified using "conv_id" column value.
All columns are strings, except for call_duration which is an integer.

1) Step 1 - preprocessing step - remove all spanish language rows - reject_spanish()
There is an initial pre-processing step. Any row that contains the word 'gracias' in the column 'plain_whisper' will be completely ignored and is not part of the data set. Encapsulate the functionality to reject 'gracias' as a pre-processing function called 'reject_spanish'. No rejected rows will be used during subsequent processing.

The data from this first step will be used in all subsequent steps.

Write the functions "getTSVData(filename)" and "rejectSpanish()", noting that the output of getTSVData will be fed as input to "rejectSpanish". In turn the output from "rejectSpanish" will be used in subseqent step. For generality, enable getTSVData to take a column separator character "sep" as a second input. "sep" will default to the tab characer.
======================================== Preprocessing ===

import pandas as pd

#=======================================================================
# Read TSV file and return pandas df that excludes spanish language rows
#=======================================================================
def getTSVData(filename, sep='\t'):
    """ 
    This function reads a TSV file into a pandas DataFrame.

    Parameters: 
    filename (str): The path to the TSV file.
    sep (str): The column separator character. It defaults to the tab character.

    Returns: 
    DataFrame: The data from the TSV file.
    """
    data = pd.read_csv(filename, sep=sep)
    return data

def rejectSpanish(data):
    """ 
    This function rejects rows with the word 'gracias' in the 'plain_whisper' column.

    Parameters: 
    data (DataFrame): The data from the TSV file.

    Returns: 
    DataFrame: The data without rows containing 'gracias' in the 'plain_whisper' column.
    """
    data = data[data['plain_whisper'].apply(lambda x: 'gracias' not in str(x).lower())]
    return data

>>>> usage
filename = 'raw_data.tsv'
data = getTSVData(filename)
data = rejectSpanish(data)
#print(data.head())

==================================== Token distribution by conversation type

Given the dataframe from rejectSpanish, create a table of values which will be visualized. The table will comprise the columns:
conv_id (from dataframe)
complaint_disat (from dataframe)
reason_level_1
token
count
density

where:
token is an individual token or word found in the conversation
count is the number of times that the token appears in the conversation
density is the count divided by the total number of tokens in the conversation

Tokens are the individual words in the conversation after all punctuation marks are removed. Tokens are typically separated by one or more spaces.

write the function "conversationTokenDensity" that will return a list of rows with columns stated above. Order the list by the token density.

write another function "densityVsComplaintHeatmap" that will display a heatmap of the 20 (or topK) tokens against "complaint_disat" column values. Use seaborn so that both the density value and color are used to show the distribution.

write another function to write the columns to file. The filename will be "tokenDensity.csv" (comma-separated). It will have a header based on the columns:
conv_id (from dataframe)
complaint_disat (from dataframe)
token
count
density

Write another group of function that generalizes the functionality above, but rather than only considering single tokens, uses bigrams (2 consecutive tokens) or ngrams (n consecutive tokens) in the calculations, heatmap, and output csv file above. Actually, it may be more beneficial to write all the functions above so that they take an additional argument that indicates how many consecutive tokens should be considered. For instance passing in "1" as the ngram value will use a single token, etc.

import nltk
import string
import itertools
from collections import Counter
from operator import itemgetter

nltk.download('punkt')

#=======================================================================
# Find token density across level_1 categories
#=======================================================================
def conversationTokenDensity(df, ngram=1):
    """
    This function calculates the token density of each conversation.
    
    Parameters:
    df (DataFrame): The data from the TSV file.
    ngram (int): The number of consecutive tokens to consider.
    
    Returns:
    DataFrame: A DataFrame with the token density of each conversation.
    """
    # Create a translation table that maps every punctuation character to None
    translator = str.maketrans('', '', string.punctuation)
    
    # Initialize an empty list to store the results
    results = []
    
    # For each row in the DataFrame
    for _, row in df.iterrows():
        # Remove punctuation and tokenize the conversation
        tokens = nltk.word_tokenize(row['plain_whisper'].translate(translator))
        
        # Generate n-grams
        ngrams = list(itertools.zip_longest(*(itertools.islice(seq, index, None) for index, seq in enumerate(itertools.tee(tokens, ngram)))))
        
        # Count the occurrences of each n-gram
        counts = Counter(ngrams)
        
        # Calculate the total number of n-grams
        total = sum(counts.values())
        
        # For each unique n-gram
        for ngram, count in counts.items():
            # Calculate the density
            density = count / total
            
            # Append the result to the list
            results.append([row['conv_id'], row['complaint_disat'], ' '.join(ngram), count, density])
    
    # Convert the list to a DataFrame and sort it by the density
    result_df = pd.DataFrame(results, columns=['conv_id', 'complaint_disat', 'token', 'count', 'density'])
    result_df = result_df.sort_values(by='density', ascending=False)
    
    return result_df
    
def writeToFile(df, ngram=1, filename=None):
    """
    This function writes a DataFrame to a CSV file.

    Parameters:
    df (DataFrame): The DataFrame to write.
    ngram (int): The number of consecutive tokens to consider.
    filename (str): The name of the file. If None, defaults to 'tokenDensity_n.csv' where n is the ngram value.

    Returns:
    None
    """
    if filename is None:
        filename = f"tokenDensity_{ngram}.csv"

    df.to_csv(filename, index=False)

-----

import seaborn as sns
import matplotlib.pyplot as plt

def densityVsComplaintHeatmap(df, ngram=1, topK=20):
    """
    This function displays a heatmap of the topK tokens or ngrams against complaint_disat column values.

    Parameters:
    df (DataFrame): The token or ngram density data.
    ngram (int): The number of consecutive tokens to consider.
    topK (int): The number of top tokens or ngrams to consider.

    Returns:
    None
    """
    # Get the topK tokens or ngrams
    top_tokens = df['token'].value_counts().index[:topK]
    
    # Filter the DataFrame to only include the topK tokens or ngrams
    df_topK = df[df['token'].isin(top_tokens)]
    
    # Create a pivot table
    pivot = df_topK.pivot_table(values='density', index='token', columns='complaint_disat')
    
    # Create a heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot, annot=True, cmap="YlGnBu")
    plt.title(f'{ngram}-gram Density vs Complaint Disatisfaction')
    plt.show()

======================== More general heatmaps

Again using the dataframe from rejectSpanish as the starting point, create a heatmap of topK token n-gram density ACROSS ALL CONVERSATIONS vs. the values in "complaint_disat" column

Create another heatmap of topK token n-gram density (top k most dense tokens) across all conversations vs. the values in reason_level_1 column

-----
def overallTokenDensity(df, ngram=1):
    """
    This function calculates the overall token density across all conversations.
    
    Parameters:
    df (DataFrame): The data from the TSV file.
    ngram (int): The number of consecutive tokens to consider.
    
    Returns:
    DataFrame: A DataFrame with the overall token density.
    """
    # Create a translation table that maps every punctuation character to None
    translator = str.maketrans('', '', string.punctuation)
    
    # Initialize an empty list to store the tokens
    tokens = []
    
    # For each row in the DataFrame
    for _, row in df.iterrows():
        # Remove punctuation and tokenize the conversation
        tokens.extend(nltk.word_tokenize(row['plain_whisper'].translate(translator)))
    
    # Generate n-grams
    ngrams = list(itertools.zip_longest(*(itertools.islice(seq, index, None) for index, seq in enumerate(itertools.tee(tokens, ngram)))))
    
    # Count the occurrences of each n-gram
    counts = Counter(ngrams)
    
    # Calculate the total number of n-grams
    total = sum(counts.values())
    
    # Initialize an empty list to store the results
    results = []
    
    # For each unique n-gram
    for ngram, count in counts.items():
        # Calculate the density
        density = count / total
        
        # Append the result to the list
        results.append([' '.join(ngram), count, density])
    
    # Convert the list to a DataFrame and sort it by the density
    result_df = pd.DataFrame(results, columns=['token', 'count', 'density'])
    result_df = result_df.sort_values(by='density', ascending=False)
    
    return result_df

----

def columnVsTokenDensity(df, density_df, column, ngram=1, topK=20):
    """
    This function calculates the token density for each unique value in a column.

    Parameters:
    df (DataFrame): The original data.
    density_df (DataFrame): The overall token density data.
    column (str): The name of the column.
    ngram (int): The number of consecutive tokens to consider.
    topK (int): The number of top tokens or ngrams to consider.

    Returns:
    DataFrame: A DataFrame with the token density for each unique value in the column.
    """
    # Get the topK tokens
    top_tokens = density_df['token'].head(topK)
    
    # Initialize an empty list to store the results
    results = []
    
    # For each unique value in the column
    for value in df[column].unique():
        # Filter the DataFrame to only include rows where the column equals the value
        value_df = df[df[column] == value]
        
        # Calculate the token density
        token_density = overallTokenDensity(value_df, ngram)
        
        # Filter the token density to only include the topK tokens
        token_density = token_density[token_density['token'].isin(top_tokens)]
        
        # For each row in the token density
        for _, row in token_density.iterrows():
            # Append the result to the list
            results.append([value, row['token'], row['count'], row['density']])
    
    # Convert the list to a DataFrame
    result_df = pd.DataFrame(results, columns=[column, 'token', 'count', 'density'])
    
    return result_df

-----

def createHeatmap(df, column, ngram=1, topK=20):
    """
    This function creates a heatmap of the token density against a column.

    Parameters:
    df (DataFrame): The original data.
    column (str): The name of the column.
    ngram (int): The number of consecutive tokens to consider.
    topK (int): The number of top tokens or ngrams to consider.

    Returns:
    None
    """
    # Calculate the overall token density
    density_df = overallTokenDensity(df, ngram)
    
    # Calculate the token density for each unique value in the column
    column_density = columnVsTokenDensity(df, density_df, column, ngram, topK)
    
    # Create a pivot table
    pivot = column_density.pivot_table(values='density', index='token', columns=column)
    
    # Create a heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot, annot=True, cmap="YlGnBu")
    plt.title(f'{ngram}-gram Density vs {column}')
    plt.show()

# Create heatmaps for 'complaint_disat' and 'reason_level_1'
createHeatmap(df, 'complaint_disat')
createHeatmap(df, 'reason_level_1')

============================== most frequently occurring n-grams for each complaint type

Again starting with the dataframe from rejectSpanish, what n-grams occur most frequently (by raw count) for each value of reason_level_1

Modify this function to include arg topK such that at most topK most frequent n-grams are returned for each unique value in reason_level_1

----

def mostFrequentNGrams(df, column='reason_level_1', ngram=1, topK=1):
    """
    This function calculates the most frequent n-grams for each unique value in a column.

    Parameters:
    df (DataFrame): The original data.
    column (str): The name of the column.
    ngram (int): The number of consecutive tokens to consider.
    topK (int): The number of top frequent ngrams to return for each unique value in the column.

    Returns:
    DataFrame: A DataFrame with the most frequent n-grams for each unique value in the column.
    """
    # Initialize an empty list to store the results
    results = []
    
    # For each unique value in the column
    for value in df[column].unique():
        # Filter the DataFrame to only include rows where the column equals the value
        value_df = df[df[column] == value]
        
        # Calculate the token density
        token_density = overallTokenDensity(value_df, ngram)
        
        # Sort the DataFrame by the count in descending order and get the topK rows
        most_frequent = token_density.sort_values(by='count', ascending=False).head(topK)
        
        # For each row in the most frequent n-grams
        for _, row in most_frequent.iterrows():
            # Append the result to the list
            results.append([value, row['token'], row['count']])
    
    # Convert the list to a DataFrame
    result_df = pd.DataFrame(results, columns=[column, 'token', 'count'])
    
    return result_df

-----

Rather than using token density, use the raw token count to re-implement mostFrequentNGrams

----
def rawTokenCount(df, ngram=1):
    """
    This function calculates the raw token count for a DataFrame.

    Parameters:
    df (DataFrame): The data.
    ngram (int): The number of consecutive tokens to consider.

    Returns:
    DataFrame: A DataFrame with the token and count.
    """
    # Get the text data
    text_data = df['plain_whisper'].values

    # Tokenize the text data and create n-grams
    ngrams = [ngram for text in text_data for ngram in generate_ngrams(text, ngram)]
    
    # Calculate the raw token count
    token_count = Counter(ngrams)
    
    # Convert the counter to a DataFrame
    count_df = pd.DataFrame.from_dict(token_count, orient='index').reset_index()
    count_df.columns = ['token', 'count']
    
    return count_df


def mostFrequentNGrams(df, column='reason_level_1', ngram=1, topK=1):
    """
    This function calculates the most frequent n-grams for each unique value in a column.

    Parameters:
    df (DataFrame): The original data.
    column (str): The name of the column.
    ngram (int): The number of consecutive tokens to consider.
    topK (int): The number of top frequent ngrams to return for each unique value in the column.

    Returns:
    DataFrame: A DataFrame with the most frequent n-grams for each unique value in the column.
    """
    # Initialize an empty list to store the results
    results = []
    
    # For each unique value in the column
    for value in df[column].unique():
        # Filter the DataFrame to only include rows where the column equals the value
        value_df = df[df[column] == value]
        
        # Calculate the raw token count
        token_count = rawTokenCount(value_df, ngram)
        
        # Sort the DataFrame by the count in descending order and get the topK rows
        most_frequent = token_count.sort_values(by='count', ascending=False).head(topK)
        
        # For each row in the most frequent n-grams
        for _, row in most_frequent.iterrows():
            # Append the result to the list
            results.append([value, row['token'], row['count']])
    
    # Convert the list to a DataFrame
    result_df = pd.DataFrame(results, columns=[column, 'token', 'count'])
    
    return result_df

XXXXXXXXXXXXXXXXXXXXXXXXXXXX highlight sentences in a text passage XXXXXXXXXXXXXXXX
I want to do some visualization around the conversations that have been analyzed so far.
The visualization code will be in javascript. The analysis and results will be on the server side. Server side will be based on Python Fast. The server should be accessible to all IP addresses and should run on port 9595.
Here is the specification of the UI:
There will be 4 main areas on the screen
top - horizontal container across the top of the screen (width == screen width)
main - remaining area below the top
main consists of 2 main areas (left and right), separated by a vertical line (about 3 pixels thick and dark green color) that starts from just below end of top, and runs to bottom of main.
right (70% of the screen width) - split into two areas by a horizontal line.
right_above - upper part of right will be 80% of the height of right. Will contain text that will come from the server.
right_below - will contain additional information from server
left - will be 30% of the screen width. Will contain a vertical stacked list of "flat" buttons. When a button on the list is clicked, it will trigger a call to the server (for instance through a "fetch"). The results of the fetch will be in json format. Each element of the json (ui.top, ui.right_above, ui.right_below) will be displayed in top, right-above, or left-above depending on a "mappingDictionary" element which will be returned as part of the response from the server. mappingDictionary will look like this:
{
	"ui": { "top":"data for top",
	  "right_above": "text for right_above area",
	  "right_below": "text for right_below area"
	}
}

At the start of the web application, the javascript code will call the server to get the list of names to be used for the buttons on the left area of the UI (the "flat" buttons). This json will be returned from the server for the initial call:

{
	"ui": { "left": comma separated list of values each representing the name of a button
	}
}

Write the UI code in javascript, minimizing the use of external js libraries, but leveraging bootstrap and CSS as much as possible. Also be very modular in the implementation. The server side will be a separate implementation. The focus here is on the client side. Organize the elements into html, css, js, and images folders.

Also:
We want 2 areas (top_left, and top_right) on the top navbar. There should be a div on the left and a drop down menu on the right. Depending on the item selected from the drop down, then the top_left, main_right_above, main_right_below will be populated by the json returned from calling the server. Basically, selecting an item from the dropdown will trigger a fetch to the server that then populates the UI elements specified.

And:
A scroll bar will be needed on the "left" since the list of items to create the buttons will be long. A scroll bar might also be needed on "right_above" as well. The list of items for "left" will be a comma separated list with a structure like this:
{
  "ui" : { "left":comma separated list of strings to be used as button labels }
}

Additionally: the area "right_above" may sometimes be used to display an image
Note that an image may be used in "right_above" if the string has .png or .jpg extension. The server will have added the image to its images folder. For example:
http://server-host:9595/images/image1.png

The buttons on the "left" area should also be triggered by arrow keys entering, so that the keyboard can be used (in addition to mouse clicks) for triggering the action of the buttons. Please include this event handling to work the same as mouse clicks.

---- client implementation/my-app
  /html
    index.html
  /css
    style.css
  /js
    main.js
  /images
    // Your images here

---- html
<!DOCTYPE html>
<html>
<head>
  <title>Application</title>
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" rel="stylesheet">
  <link href="../css/style.css" rel="stylesheet">
</head>
<body>
  <nav class="navbar navbar-expand-lg navbar-light bg-light justify-content-between">
    <div id="top_left" class="p-2 top-left-class">
      <!-- Content dynamically added by JavaScript -->
    </div>
    <div class="dropdown">
      <button class="btn btn-secondary dropdown-toggle" type="button" id="dropdownMenuButton" data-toggle="dropdown">
        Select
      </button>
      <div id="dropdown-menu" class="dropdown-menu">
        <!-- Dropdown options dynamically added by JavaScript -->
      </div>
    </div>
  </nav>
  <div id="main" class="container-fluid">
    <div class="row">
      <div id="left" class="col-4 p-3 left-class"></div>
      <div id="right" class="col-8 p-3 right-class">
        <div id="right_above" class="right-above-class"></div>
        <div id="right_below" class="right-below-class"></div>
      </div>
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"></script>
  <script src="../js/main.js" defer></script>
</body>
</html>

---- css
body {
  display: flex;
  flex-direction: column;
  height: 100vh;
}

#top {
  height: 10%;
  width: 100%;
  background-color: lightgray;
}

#main {
  display: flex;
  height: 90%;
  width: 100%;
}

#left {
  width: 30%;
  border-right: 3px solid darkgreen;
}

#right {
  width: 70%;
  display: flex;
  flex-direction: column;
}

#right_above {
  height: 80%;
  border-bottom: 3px solid darkgreen;
}

#right_below {
  height: 20%;
}

.top-class {
  background-color: lightgray;
}

.top-left-class {
  flex-grow: 1;
}

.left-class {
  border-right: 3px solid darkgreen;
}

.right-class {
  display: flex;
  flex-direction: column;
}

.right-above-class {
  height: 80%;
  border-bottom: 3px solid darkgreen;
}

.right-below-class {
  height: 20%;
}

.left-class {
  border-right: 3px solid darkgreen;
  overflow-y: auto; /* This makes the area scrollable in the y-direction */
  height: calc(100vh - 56px); /* Adjust as necessary */
}

.right-above-class {
  height: 80%;
  border-bottom: 3px solid darkgreen;
  overflow-y: auto; /* This makes the area scrollable in the y-direction */
}

---- js
window.onload = async function() {
  // Fetch initial data for dropdown options and button labels
  let response = await fetch("http://your-server-address:9595/initial-data");
  let data = await response.json();

  // Use data to populate left section with buttons
  let buttonLabels = data.ui.left.split(",");
  let leftSection = document.getElementById("left");
  buttonLabels.forEach(label => {
    let button = document.createElement("button");
    button.textContent = label;
    button.className = "btn btn-primary mb-2"; // Bootstrap classes for basic styling
    leftSection.appendChild(button);
  });

  // Add keydown event listener after button creation
  addKeyboardNavigation();

  // Use data to populate dropdown with options
  let dropdownOptions = data.ui.dropdown.split(",");
  let dropdownMenu = document.getElementById("dropdown-menu");
  dropdownOptions.forEach(option => {
    let dropdownItem = document.createElement("a");
    dropdownItem.className = "dropdown-item";
    dropdownItem.href = "#";
    dropdownItem.textContent = option;
    dropdownItem.onclick = handleDropdownSelection;
    dropdownMenu.appendChild(dropdownItem);
  });
};

// Add keyboard navigation functionality
function addKeyboardNavigation() {
  document.body.addEventListener('keydown', function(e) {
    const leftArea = document.getElementById('left');
    const buttons = leftArea.getElementsByTagName('button');

    let activeButton = document.activeElement.tagName === 'BUTTON' ? document.activeElement : buttons[0];
    let activeButtonIndex = Array.from(buttons).indexOf(activeButton);

    switch (e.code) {
      case 'ArrowUp':
        if (activeButtonIndex > 0) {
          activeButtonIndex--;
          buttons[activeButtonIndex].focus();
        }
        break;
      case 'ArrowDown':
        if (activeButtonIndex < buttons.length - 1) {
          activeButtonIndex++;
          buttons[activeButtonIndex].focus();
        }
        break;
      case 'Enter':
        activeButton.click();
        break;
    }
  });
}

async function handleDropdownSelection(e) {
  e.preventDefault();

  let dropdownSelection = e.target.textContent;

  // Fetch data from server based on dropdown selection
  let response = await fetch(`http://your-server-address:9595/data?dropdown=${dropdownSelection}`);
  let data = await response.json();

  // Use data to populate areas
  document.getElementById("top_left").textContent = data.ui.top_left;

  // If right_above data is an image URL, create an img element. Otherwise, treat it as text
  let rightAboveData = data.ui.right_above;
  let rightAboveElement = document.getElementById("right_above");
  rightAboveElement.innerHTML = ''; // Clear previous content
  if (rightAboveData.endsWith('.png') || rightAboveData.endsWith('.jpg')) {
    let img = document.createElement('img');
    img.src = rightAboveData;
    img.alt = 'Image';
    img.style.width = '100%'; // to ensure the image fits the container
    rightAboveElement.appendChild(img);
  } else {
    rightAboveElement.textContent = rightAboveData;
  }

  document.getElementById("right_below").textContent = data.ui.right_below;
}

=======
-----
Given a tab separated file "raw_data.tsv" that has the following columns (specified in the header):
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
reason_level_2
plain_whisper
language_code

Write a function that will return the number of distinct strings in a column.
-------
import pandas as pd

def count_unique_strings(filename, column):
    try:
        # Read the TSV file
        df = pd.read_csv(filename, sep='\t')

        # Check if the column exists in the DataFrame
        if column not in df.columns:
            return f"Error: Column '{column}' not found in the data."

        # Count the unique strings in the specified column
        unique_strings = df[column].nunique()

        return unique_strings

    except FileNotFoundError:
        return f"Error: File '{filename}' not found."

    except Exception as e:
        return f"Error: {str(e)}"
--
filename = "raw_data.tsv"
column = "conv_id"
print(count_unique_strings(filename, column))

==================== server side
Implement the server side using Python Fast. Include CORS handling, and make it possible for any IP address to access the server.

Implement the following GETs :
getDropdownOptions() - return a json of items that will be used to populate the top_right dropdown
{
	"ui": { "top_right": comma separated list of values each representing an item on the dropdown list
	}
}

getConvIDs() - return a list of strings that will be used as button labels on the "left" of the UI. The list of button labels should be returned as follows:
{
  "ui" : { "left":comma separated list of strings to be used as button labels }
}

handleButtonClick(buttonLabel) - return the ui elements to be used in each of the UI areas. 
do some processing
return results in json format like:
{
	"ui": { "top":"data for top",
	  "right_above": "text or image for right_above area",
	  "right_below": "text for right_below area"
	}
}

----
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

app = FastAPI()

origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/getDropdownOptions")
async def get_dropdown_options():
    # TODO: replace with actual implementation
    return {
        "ui": {
            "top_right": "option1,option2,option3"
        }
    }

@app.get("/getConvIDs")
async def get_conv_ids():
    # TODO: replace with actual implementation
    return {
        "ui": {
            "left": "label1,label2,label3"
        }
    }

@app.get("/handleButtonClick/{button_label}")
async def handle_button_click(button_label: str):
    # TODO: replace with actual implementation
    return {
        "ui": {
            "top": "data for top",
            "right_above": "text or image for right_above area",
            "right_below": "text for right_below area"
        }
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=9595)