Given a number of csv files, all with the following structure:
step, accuracy, precision, recall, f1

where the first line is the header above, and subsequent lines are values for each column.

Given n filenames, create 4 separate line graphs that compares the values of each of the columns:
accuracy
precision
recall
f1

So for example, if there are 3 csv files, then there will be 4 separate graphs that each show 3 lines, one for the values of the relevant column in each file.

Also, write the python function in such a way that it is easy to associate each filename with a label that will be used in the legend for its line.

========================================

import pandas as pd
import matplotlib.pyplot as plt

def plot_csv_data(filenames, labels):
    # Initialize an empty dictionary to store the data for each column
    data = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}

    # Read and process each CSV file
    for filename in filenames:
        # Read the CSV file into a DataFrame
        df = pd.read_csv(filename)

        # Append the values of each column to the corresponding list in the data dictionary
        data['accuracy'].append(df['accuracy'])
        data['precision'].append(df['precision'])
        data['recall'].append(df['recall'])
        data['f1'].append(df['f1'])

    # Create separate line graphs for each column
    for column in data:
        # Create a new figure
        plt.figure()

        # Plot each line with the corresponding label
        for i, values in enumerate(data[column]):
            plt.plot(values, label=labels[i])

        # Set the title and labels for the graph
        plt.title(column)
        plt.xlabel('Step')
        plt.ylabel(column)

        # Add a legend to the graph
        plt.legend()

        # Display the graph
        plt.show()

----------------

filenames = ['file1.csv', 'file2.csv', 'file3.csv']
labels = ['File 1', 'File 2', 'File 3']

plot_csv_data(filenames, labels)

=================================================
Given a tab separated file "raw_data.tsv" that has the following columns (specified in the header):
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
reason_level_2
plain_whisper
language_code

Each row represents a conversation that identified using "conv_id" column value.
All columns are strings, except for call_duration which is an integer.

1) Step 1 - preprocessin step - remove all spanish language rows - reject_spanish()
There is an initial pre-processing step. Any row that contains the word 'gracias' in the column 'plain_whisper' will be completely ignored and is not part of the data set. Encapsulate the functionality to reject 'gracias' as a pre-processing function called 'reject_spanish'. No rejected rows will be used during subsequent processing. Any other robust method to identify and reject spanish language rows can also be used.

2) Step 2 - find top k tf-idf scores: level_1_tf_idf(phrase_length, top_k), where:
phrase_length is number of tokens in ngram
top_k is the top n number of phrases (eg. top 10 phrases by tf-idf score)
'level_1_tf_idf' will find the top 10 scoring tf-idf phrases in each group of 'plain_whisper' columns that belong to the same 'reason_level_1' where the value of the 'complaint_disat' column is 'complaint'. The length of a phrase is an input argument to the tf-idf function. The phrase will come from the column "plain_whisper". The tf-idf calculation will be done across the set of 'plain_whisper' columns identified in pre-processing step 1.  The output will be a key-value array where the key is the value of 'reason_level_1' column and the value is the top 10 phrases, each with a tf-idf score. Basically we want to find out the most frequently occurring phrases in 'plain_whisper' column for rows with the same 'reason_level_1'.

3) Step 3 - get most significant 3 dimensions for each phrase based on mean Word2Vec representation of the phrase: get_mean_vector_score()
Using the Word2Vec score for each top 10 phrase, find the most significant 3 dimensions for each phrase. This should be the 3 dimensions for all phrases that is most significant in the sense of PCA or SVD

4) Step 4 - Do a 3D visualization of the phrases, using dots color-coded by 'reason_level_1' for each phrase. Basically all phrases that belong to the same 'reason_level_1' will have the same color. Display this plot.

5) Step 5 - Do a 3D visualization as in step 4, but only display the centroid position of each group of dots using a single dot of the same color. Label the dot with the 'reason_level_1'. Display this plot.

===========================

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from gensim.models import Word2Vec
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from nltk import word_tokenize
import numpy as np

def reject_spanish(filepath):
    data = pd.read_csv(filepath, sep='\t')
    data = data[~data['plain_whisper'].str.contains('gracias', na=False)]
    return data

def level_1_tf_idf(data, phrase_length, top_k, global_idf=False):
    # Find the top scoring tf-idf phrases for each 'reason_level_1' group.
    phrase_vectors = {}
    vectorizer = TfidfVectorizer(ngram_range=(phrase_length, phrase_length))

    for level, group in data.groupby('reason_level_1'):
        tfidf_matrix = vectorizer.fit_transform(group['plain_whisper'] if global_idf else data['plain_whisper'])
        scores = tfidf_matrix.sum(axis=0).A1
        top_indices = scores.argsort()[-top_k:]
        feature_names = vectorizer.get_feature_names_out()
        top_phrases = [feature_names[idx] for idx in top_indices]

        # Compute the mean GloVe vector for each top phrase
        vectors = np.array([get_phrase_vector(phrase) for phrase in top_phrases])
        phrase_vectors[level] = vectors

    return phrase_vectors

def get_mean_vector_score(data, top_phrases):
    tokenized_sentences = data['plain_whisper'].apply(word_tokenize).tolist()
    model = Word2Vec(tokenized_sentences, min_count=1)
    phrase_vectors = {}

    for level, phrases in top_phrases.items():
        level_vectors = []
        for phrase in phrases:
            word_vectors = [model.wv[word] for word in word_tokenize(phrase) if word in model.wv]
            if word_vectors:
                level_vectors.append(np.mean(word_vectors, axis=0))
        phrase_vectors[level] = level_vectors

    svd = TruncatedSVD(n_components=3)
    for level, vectors in phrase_vectors.items():
        if vectors:
            phrase_vectors[level] = svd.fit_transform(vectors)

    return phrase_vectors

def visualize(phrase_vectors, centroids=False, filename='visualization.png'):
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']

    for i, (level, vectors) in enumerate(phrase_vectors.items()):
        if vectors.any():
            xs, ys, zs = vectors.T
            if centroids:
                centroid = np.mean(vectors, axis=0)
                ax.scatter(*centroid, c=colors[i%7], label=level)
            else:
                ax.scatter(xs, ys, zs, c=colors[i%7], label=level)

    ax.legend()
    plt.show()

    # Save the figure as a PNG image
    fig.savefig(filename)

# Read and preprocess data
filepath = "raw_data.tsv"
data = reject_spanish(filepath)

# Find top 10 phrases by tf-idf score
phrase_length = 2
top_k = 10
global_idf = False  # change this to True to calculate IDF globally
top_phrases = level_1_tf_idf(data, phrase_length, top_k, global_idf)

# Print top phrases and write to a file
with open('top_tf_idf_phrases.txt', 'w') as f:
    for reason, phrases in top_phrases.items():
        for phrase in phrases:
            print(f"{reason}: {phrase}")
            f.write(f"{reason}: {phrase}\n")

# Get most significant 3 dimensions for each phrase
phrase_vectors = get_mean_vector_score(data, top_phrases)

# Visualize the phrases
visualize(phrase_vectors)

# Visualize the centroids
visualize(phrase_vectors, centroids=True)

-------------------- using torchtext instead of Word2Vec

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from torchtext.vocab import Vectors
import os

def reject_spanish(data):
    # Remove rows containing the word 'gracias' in the 'plain_whisper' column.
    data = data[~data['plain_whisper'].str.contains('gracias')]
    return data


def get_phrase_vector(phrase):
    # Return the mean GloVe vector for the given phrase.
    words = phrase.split()
    vectors = np.stack([glove.vectors[glove.stoi[word]] for word in words if word in glove.stoi])
    vector_mean = np.mean(vectors, axis=0)
    return vector_mean


def level_1_tf_idf(data, phrase_length, top_k, global_idf=False):
    # Find the top scoring tf-idf phrases for each 'reason_level_1' group.
    phrase_vectors = {}
    vectorizer = TfidfVectorizer(ngram_range=(phrase_length, phrase_length))

    for level, group in data.groupby('reason_level_1'):
        tfidf_matrix = vectorizer.fit_transform(group['plain_whisper'] if global_idf else data['plain_whisper'])
        scores = tfidf_matrix.sum(axis=0).A1
        top_indices = scores.argsort()[-top_k:]
        feature_names = vectorizer.get_feature_names_out()
        top_phrases = [feature_names[idx] for idx in top_indices]

        # Compute the mean GloVe vector for each top phrase
        vectors = np.array([get_phrase_vector(phrase) for phrase in top_phrases])
        phrase_vectors[level] = (top_phrases, vectors)

    svd = TruncatedSVD(n_components=3)
    phrase_vectors_svd = {}
    for level, (phrases, vectors) in phrase_vectors.items():
        if vectors.size > 0:  # add condition to avoid empty vectors
            vectors_svd = svd.fit_transform(vectors)
            phrase_vectors_svd[level] = (phrases, vectors_svd)
    
    return phrase_vectors_svd


def visualize(phrase_vectors, centroids=False, filename=None):
    # Visualize the phrases or centroids in a 3D plot, color-coded by 'reason_level_1'.
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')

    colors = ['r', 'g', 'b', 'c', 'm', 'y', 'orange', 'purple', 'lime', 'cyan', 'pink', 'gold', 'navy', 'teal']
    label_to_color = {}  # Mapping of phrase labels to colors

    unique_labels = list(phrase_vectors.keys())  # Get the unique labels

    for idx, level in enumerate(unique_labels):
        phrases, vectors = phrase_vectors[level]
        color = colors[idx % len(colors)]  # Cycle through colors
        label_to_color[level] = color  # Assign unique color for each phrase label
        ax.scatter(vectors[:, 0], vectors[:, 1], vectors[:, 2], c=color, label=level, marker='.', s=200)

    ax.set_xlabel('Dimension 1')
    ax.set_ylabel('Dimension 2')
    ax.set_zlabel('Dimension 3')

    if centroids:
        centroid_vectors = {level: np.mean(vectors, axis=0) for level, (phrases, vectors) in phrase_vectors.items()}
        for level, vector in centroid_vectors.items():
            if level in phrase_vectors:
                color = label_to_color[level]
                ax.scatter(vector[0], vector[1], vector[2], c=color, label=level, marker='.', s=200)

    # Place legend to the right of the figure
    ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1))

    if filename:
        plt.savefig(filename, bbox_inches='tight')  # Ensure the entire legend is included in the saved figure

    plt.show()

def visualize_split(phrase_vectors, numLevels=8, centroids=False, filename=None):
    # Visualize the phrases or centroids in a 3D plot, color-coded by 'reason_level_1'.
    colors = ['r', 'g', 'c', 'm', 'y', 'orange', 'purple', 'lime', 'cyan', 'pink', 'gold', 'navy', 'teal']

    unique_labels = list(phrase_vectors.keys())  # Get the unique labels
    if 'Not a Complaint' in unique_labels:
        unique_labels.remove('Not a Complaint')  # Remove 'Not a Complaint' from the list

    # Create group of levels
    level_groups = [unique_labels[n:n+(numLevels-1)] for n in range(0, len(unique_labels), numLevels-1)]

    for i, levels in enumerate(level_groups):
        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')

        # Always include 'Not a Complaint' and assign it the color blue
        label_to_color = {'Not a Complaint': 'b'}
        ax.scatter(phrase_vectors['Not a Complaint'][1][:, 0], phrase_vectors['Not a Complaint'][1][:, 1], 
                   phrase_vectors['Not a Complaint'][1][:, 2], c='b', label='Not a Complaint', marker='.', s=200)

        for idx, level in enumerate(levels):
            color = colors[idx % len(colors)]
            label_to_color[level] = color  # Assign unique color for each phrase label
            vectors = phrase_vectors[level][1]
            ax.scatter(vectors[:, 0], vectors[:, 1], vectors[:, 2], c=color, label=level, marker='.', s=200)

        ax.set_xlabel('Dimension 1')
        ax.set_ylabel('Dimension 2')
        ax.set_zlabel('Dimension 3')

        if centroids:
            centroid_vectors = {level: np.mean(vectors, axis=0) for level, (phrases, vectors) in phrase_vectors.items() if level in levels+['Not a Complaint']}
            for level, vector in centroid_vectors.items():
                if level in phrase_vectors:
                    color = label_to_color[level]
                    ax.scatter(vector[0], vector[1], vector[2], c=color, label=level, marker='.', s=200)

        # Move plot to the left
        plt.subplots_adjust(left=-0.25)  # Adjust this value to move the plot more to the left

        # Place legend to the right of the figure
        ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1))

        if filename:
            filename_base, filename_ext = os.path.splitext(filename)
            plt.savefig(f"{filename_base}_{i}{filename_ext}", bbox_inches='tight')  # Ensure the entire legend is included in the saved figure

        plt.show()



# GloVe model file
glove_file = "/chunk-complaint/category/glove/glove.6B.300d"

# Load GloVe vectors with torchtext
glove = Vectors(name=glove_file)

# Load the data
data = pd.read_csv('raw_data.tsv', sep='\t')

# Preprocess the data
data = reject_spanish(data)

# Compute tf-idf scores
phrase_vectors = level_1_tf_idf(data, phrase_length=2, top_k=10, global_idf=True)

# Write the results to file
with open('tf-idf-results.txt', 'w') as f:
    for level, (top_phrases, vectors) in phrase_vectors.items():
        for idx, phrase in enumerate(top_phrases):
            tfidf_score = vectors[idx][1]  # Get the tf-idf score from the vector
            f.write(f'"{level}", "{phrase}", {tfidf_score}\n')

# Visualize the phrases
visualize(phrase_vectors, filename='phrase_visualization.png')

# Visualize the centroids
visualize(phrase_vectors, centroids=True, filename='centroid_visualization.png')

visualize_split(phrase_vectors, numLevels=8, filename='phrase_visualization.png')
visualize_split(phrase_vectors, centroids=True, numLevels=8, filename='centroid_visualization.png')


-------------- additional
The 3D plot is very crowded. Let's keep the current plots, but add another set of 3D plots that will be easier to read. Create the new 3D plots as follows", each of which should contain the 'level' with the label 'Not a Complaint' since we want to see the relationship of other levels to 'Not a Complaint'. Limit the number of levels to be displayed in each plot to numLevels (this should be configurable, but you can use a default of 8 -- inclusive of the required 'Not a Complaint'). Generate as many plots required to cover all levels with no more than numLevels per plot. Move the plot to the left as there is a lot of whitespace there and keep the legend outsid the plot as usual.

------------ additional 2
Always show 'Not a Complaint' in blue. Then use any other unique colors for the other labels. Move the graph even more to the left.

------------ additional 3
There seems to be a slight misunderstanding. 'Not a Complaint' has to be present on each 3D plot because we want to compare the position of its points with the positions of the points of other labels. Also, the points used to label 'Not a Complaint' have to be blue. Other labels can then use other available colors.

