Given a number of csv files, all with the following structure:
step, accuracy, precision, recall, f1

where the first line is the header above, and subsequent lines are values for each column.

Given n filenames, create 4 separate line graphs that compares the values of each of the columns:
accuracy
precision
recall
f1

So for example, if there are 3 csv files, then there will be 4 separate graphs that each show 3 lines, one for the values of the relevant column in each file.

Also, write the python function in such a way that it is easy to associate each filename with a label that will be used in the legend for its line.

========================================

import pandas as pd
import matplotlib.pyplot as plt

def plot_csv_data(filenames, labels):
    # Initialize an empty dictionary to store the data for each column
    data = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}

    # Read and process each CSV file
    for filename in filenames:
        # Read the CSV file into a DataFrame
        df = pd.read_csv(filename)

        # Append the values of each column to the corresponding list in the data dictionary
        data['accuracy'].append(df['accuracy'])
        data['precision'].append(df['precision'])
        data['recall'].append(df['recall'])
        data['f1'].append(df['f1'])

    # Create separate line graphs for each column
    for column in data:
        # Create a new figure
        plt.figure()

        # Plot each line with the corresponding label
        for i, values in enumerate(data[column]):
            plt.plot(values, label=labels[i])

        # Set the title and labels for the graph
        plt.title(column)
        plt.xlabel('Step')
        plt.ylabel(column)

        # Add a legend to the graph
        plt.legend()

        # Display the graph
        plt.show()

----------------

filenames = ['file1.csv', 'file2.csv', 'file3.csv']
labels = ['File 1', 'File 2', 'File 3']

plot_csv_data(filenames, labels)

=================================================
Given a tab separated file "raw_data.tsv" that has the following columns (specified in the header):
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
reason_level_2
plain_whisper
language_code

Each row represents a conversation that identified using "conv_id" column value.
All columns are strings, except for call_duration which is an integer.

1) Step 1 - preprocessin step - remove all spanish language rows - reject_spanish()
There is an initial pre-processing step. Any row that contains the word 'gracias' in the column 'plain_whisper' will be completely ignored and is not part of the data set. Encapsulate the functionality to reject 'gracias' as a pre-processing function called 'reject_spanish'. No rejected rows will be used during subsequent processing. Any other robust method to identify and reject spanish language rows can also be used.

2) Step 2 - find top k tf-idf scores: level_1_tf_idf(phrase_length, top_k), where:
phrase_length is number of tokens in ngram
top_k is the top n number of phrases (eg. top 10 phrases by tf-idf score)
'level_1_tf_idf' will find the top 10 scoring tf-idf phrases in each group of 'plain_whisper' columns that belong to the same 'reason_level_1' where the value of the 'complaint_disat' column is 'complaint'. The length of a phrase is an input argument to the tf-idf function. The phrase will come from the column "plain_whisper". The tf-idf calculation will be done across the set of 'plain_whisper' columns identified in pre-processing step 1.  The output will be a key-value array where the key is the value of 'reason_level_1' column and the value is the top 10 phrases, each with a tf-idf score. Basically we want to find out the most frequently occurring phrases in 'plain_whisper' column for rows with the same 'reason_level_1'.

3) Step 3 - get most significant 3 dimensions for each phrase based on mean Word2Vec representation of the phrase: get_mean_vector_score()
Using the Word2Vec score for each top 10 phrase, find the most significant 3 dimensions for each phrase. This should be the 3 dimensions for all phrases that is most significant in the sense of PCA or SVD

4) Step 4 - Do a 3D visualization of the phrases, using dots color-coded by 'reason_level_1' for each phrase. Basically all phrases that belong to the same 'reason_level_1' will have the same color. Display this plot.

5) Step 5 - Do a 3D visualization as in step 4, but only display the centroid position of each group of dots using a single dot of the same color. Label the dot with the 'reason_level_1'. Display this plot.

===========================

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from gensim.models import Word2Vec
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from nltk import word_tokenize
import numpy as np

def reject_spanish(filepath):
    data = pd.read_csv(filepath, sep='\t')
    data = data[~data['plain_whisper'].str.contains('gracias', na=False)]
    return data

def level_1_tf_idf(data, phrase_length, top_k, global_idf=False):
    # Find the top scoring tf-idf phrases for each 'reason_level_1' group.
    phrase_vectors = {}
    vectorizer = TfidfVectorizer(ngram_range=(phrase_length, phrase_length))

    for level, group in data.groupby('reason_level_1'):
        tfidf_matrix = vectorizer.fit_transform(group['plain_whisper'] if global_idf else data['plain_whisper'])
        scores = tfidf_matrix.sum(axis=0).A1
        top_indices = scores.argsort()[-top_k:]
        feature_names = vectorizer.get_feature_names_out()
        top_phrases = [feature_names[idx] for idx in top_indices]

        # Compute the mean GloVe vector for each top phrase
        vectors = np.array([get_phrase_vector(phrase) for phrase in top_phrases])
        phrase_vectors[level] = vectors

    return phrase_vectors

def get_mean_vector_score(data, top_phrases):
    tokenized_sentences = data['plain_whisper'].apply(word_tokenize).tolist()
    model = Word2Vec(tokenized_sentences, min_count=1)
    phrase_vectors = {}

    for level, phrases in top_phrases.items():
        level_vectors = []
        for phrase in phrases:
            word_vectors = [model.wv[word] for word in word_tokenize(phrase) if word in model.wv]
            if word_vectors:
                level_vectors.append(np.mean(word_vectors, axis=0))
        phrase_vectors[level] = level_vectors

    svd = TruncatedSVD(n_components=3)
    for level, vectors in phrase_vectors.items():
        if vectors:
            phrase_vectors[level] = svd.fit_transform(vectors)

    return phrase_vectors

def visualize(phrase_vectors, centroids=False, filename='visualization.png'):
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']

    for i, (level, vectors) in enumerate(phrase_vectors.items()):
        if vectors.any():
            xs, ys, zs = vectors.T
            if centroids:
                centroid = np.mean(vectors, axis=0)
                ax.scatter(*centroid, c=colors[i%7], label=level)
            else:
                ax.scatter(xs, ys, zs, c=colors[i%7], label=level)

    ax.legend()
    plt.show()

    # Save the figure as a PNG image
    fig.savefig(filename)

# Read and preprocess data
filepath = "raw_data.tsv"
data = reject_spanish(filepath)

# Find top 10 phrases by tf-idf score
phrase_length = 2
top_k = 10
global_idf = False  # change this to True to calculate IDF globally
top_phrases = level_1_tf_idf(data, phrase_length, top_k, global_idf)

# Print top phrases and write to a file
with open('top_tf_idf_phrases.txt', 'w') as f:
    for reason, phrases in top_phrases.items():
        for phrase in phrases:
            print(f"{reason}: {phrase}")
            f.write(f"{reason}: {phrase}\n")

# Get most significant 3 dimensions for each phrase
phrase_vectors = get_mean_vector_score(data, top_phrases)

# Visualize the phrases
visualize(phrase_vectors)

# Visualize the centroids
visualize(phrase_vectors, centroids=True)

-------------------- using torchtext instead of Word2Vec

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from torchtext.vocab import Vectors

def reject_spanish(data):
    # Remove rows containing the word 'gracias' in the 'plain_whisper' column.
    data = data[~data['plain_whisper'].str.contains('gracias')]
    return data

def get_phrase_vector(phrase):
    # Return the mean GloVe vector for the given phrase.
    words = phrase.split()
    vectors = np.stack([glove.vectors[glove.stoi[word]] for word in words if word in glove.stoi])
    vector_mean = np.mean(vectors, axis=0)
    return vector_mean

def level_1_tf_idf(data, phrase_length, top_k, global_idf=False):
    # Find the top scoring tf-idf phrases for each 'reason_level_1' group where 'complaint_disat' is 'complaint'
    phrase_vectors = {}
    vectorizer = TfidfVectorizer(ngram_range=(phrase_length, phrase_length))

    for level, group in data.groupby('reason_level_1'):
        complaint_group = group[group['complaint_disat'] == 'complaint']
        tfidf_matrix = vectorizer.fit_transform(complaint_group['plain_whisper'] if global_idf else data['plain_whisper'])
        scores = tfidf_matrix.sum(axis=0).A1
        top_indices = scores.argsort()[-top_k:]
        feature_names = vectorizer.get_feature_names_out()
        top_phrases = [feature_names[idx] for idx in top_indices]

        # Compute the mean GloVe vector for each top phrase
        vectors = np.array([get_phrase_vector(phrase) for phrase in top_phrases])
        phrase_vectors[level] = vectors

	svd = TruncatedSVD(n_components=3)
	for level, vectors in phrase_vectors.items():
		if vectors:
			phrase_vectors[level] = svd.fit_transform(vectors)

    return phrase_vectors


def visualize(phrase_vectors, centroids=False, filename=None):
    # Visualize the phrases or centroids in a 3D plot, color-coded by 'reason_level_1'.
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')

    colors = ['r', 'g', 'b', 'c', 'm', 'y']
    label_to_color = {}  # Mapping of phrase labels to colors

    unique_labels = list(phrase_vectors.keys())  # Get the unique labels

    for idx, level in enumerate(unique_labels):
        vectors = phrase_vectors[level]
        label_to_color[level] = colors[idx]  # Assign unique color for each phrase label
        ax.scatter(vectors[:, 0], vectors[:, 1], vectors[:, 2], c=colors[idx], label=level, marker='x', s=200)

    ax.set_xlabel('Dimension 1')
    ax.set_ylabel('Dimension 2')
    ax.set_zlabel('Dimension 3')

    if centroids:
        centroid_vectors = {level: np.mean(vectors, axis=0) for level, vectors in phrase_vectors.items()}
        for level, vector in centroid_vectors.items():
            if level in phrase_vectors:
                vectors = phrase_vectors[level]
                color = label_to_color[level]
                ax.scatter(vector[0], vector[1], vector[2], c=color, label=level, marker='x', s=200)

    ax.legend()

    if filename:
        plt.savefig(filename)

    plt.show()

# Path to your GloVe model file
glove_file = "/chunk-complaint/category/glove/glove.6B.300d"

# Load GloVe vectors with torchtext
glove = Vectors(name=glove_file)

# Load the data
data = pd.read_csv('raw_data.tsv', sep='\t')

# Preprocess the data
data = reject_spanish(data)

# Compute tf-idf scores
phrase_vectors = level_1_tf_idf(data, phrase_length=2, top_k=10, global_idf=True)

# Visualize the phrases
visualize(phrase_vectors, filename='phrase_visualization.png')

# Visualize the centroids
visualize(phrase_vectors, centroids=True, filename='centroid_visualization.png')

---------------------

ValueError: setting an array element with a sequence. The requested array has an inhomogenous shape after 2 dimensions. The detected shape was (2,) + inhomogenous part.
around the line:
centroid_vectors = {level: np.mean(vectors, axis=0) for level, vectors  in phrase_vectors.items()}


import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from torchtext.vocab import Vectors


def reject_spanish(data):
    # Remove rows containing the word 'gracias' in the 'plain_whisper' column.
    data = data[~data['plain_whisper'].str.contains('gracias')]
    return data


def get_phrase_vector(phrase):
    # Return the mean GloVe vector for the given phrase.
    words = phrase.split()
    vectors = np.stack([glove.vectors[glove.stoi[word]] for word in words if word in glove.stoi])
    vector_mean = np.mean(vectors, axis=0)
    return vector_mean


def level_1_tf_idf(data, phrase_length, top_k, global_idf=False):
    # Find the top scoring tf-idf phrases for each 'reason_level_1' group.
    phrase_vectors = {}
    vectorizer = TfidfVectorizer(ngram_range=(phrase_length, phrase_length))

    for level, group in data.groupby('reason_level_1'):
        tfidf_matrix = vectorizer.fit_transform(group['plain_whisper'] if global_idf else data['plain_whisper'])
        scores = tfidf_matrix.sum(axis=0).A1
        top_indices = scores.argsort()[-top_k:]
        feature_names = vectorizer.get_feature_names_out()
        top_phrases = [feature_names[idx] for idx in top_indices]

        # Compute the mean GloVe vector for each top phrase
        vectors = np.array([get_phrase_vector(phrase) for phrase in top_phrases])
        phrase_vectors[level] = (top_phrases, vectors)

    return phrase_vectors


def visualize(phrase_vectors, centroids=False, filename=None):
    # Visualize the phrases or centroids in a 3D plot, color-coded by 'reason_level_1'.
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')

    colors = ['r', 'g', 'b', 'c', 'm', 'y', 'orange', 'purple', 'lime', 'cyan', 'pink', 'gold', 'navy', 'teal']
    label_to_color = {}  # Mapping of phrase labels to colors

    unique_labels = list(phrase_vectors.keys())  # Get the unique labels

    for idx, level in enumerate(unique_labels):
        phrases, vectors = phrase_vectors[level]
        color = colors[idx % len(colors)]  # Cycle through colors
        label_to_color[level] = color  # Assign unique color for each phrase label
        ax.scatter(vectors[:, 0], vectors[:, 1], vectors[:, 2], c=color, label=level, marker='x', s=200)

    ax.set_xlabel('Dimension 1')
    ax.set_ylabel('Dimension 2')
    ax.set_zlabel('Dimension 3')

    if centroids:
        centroid_vectors = {level: np.mean(vectors, axis=0) for level, (phrases, vectors) in phrase_vectors.items()}
        for level, vector in centroid_vectors.items():
            if level in phrase_vectors:
                color = label_to_color[level]
                ax.scatter(vector[0], vector[1], vector[2], c=color, label=level, marker='x', s=200)

    ax.legend()

    if filename:
        plt.savefig(filename)

    plt.show()

# Path to your GloVe model file
glove_file = "/chunk-complaint/category/glove/glove.6B.300d"

# Load GloVe vectors with torchtext
glove = Vectors(name=glove_file)

# Load the data
data = pd.read_csv('raw_data.tsv', sep='\t')

# Preprocess the data
data = reject_spanish(data)

# Compute tf-idf scores
phrase_vectors = level_1_tf_idf(data, phrase_length=2, top_k=10, global_idf=True)

# Write the results to file
with open('tf-idf-results.txt', 'w') as f:
    for level, (top_phrases, vectors) in phrase_vectors.items():
        for idx, phrase in enumerate(top_phrases):
            tfidf_score = vectors[idx][1]  # Get the tf-idf score from the vector
            f.write(f'"{level}", "{phrase}", {tfidf_score}\n')

# Visualize the phrases
visualize(phrase_vectors, filename='phrase_visualization.png')

# Visualize the centroids
visualize(phrase_vectors, centroids=True, filename='centroid_visualization.png')

-------------- debug
AttributeError: 'numpy.ndarray' object has not attribute 'numpy'
at line:    return vector_mean.numpy()

ValueError: Invalid literal for int() with base 10: '*Case ID No Longer in the CTR SQL Database'
at line:   ax.scatter(vectors[:, 0], vectors[:, 1], vectors[:, 2], c=colors[int(level)], label=level)

TypeError: list indices must be integers or slices, not str
at line:    ax.scatter(vector[0], vector[1], vector[2], c=colors[level], label=level, marker='x', s=200)

UnboundLocalError: local variable 'vector' referenced before assignment
at line:   ax.scatter(vector[0], vector[1], vector[2], c=colors[int(level)], label=level, marker='x', s=200)

IndexError: list index out of range
at line: label_to_color[level] = colors[idx]  # Assign unique color for each phrase label


after Computing the tf-idf scores:
phrase_vectors = level_1_tf_idf(data, phrase_length=2, top_k=10, global_idf=True)
also write the results to file 'tf-idf-results.txt'

use the tab separated format:
level <tab> phrase <tab> tf-idf-score
when writing the results to file
Just show the line(s) of code to do this

instead of tabs, use commas. But enclose the level in quotes

ValueError: need at least one array to stack
around line: vectors = np.stack([glove.vectors[glove.stoi[word]] for word in words if word in glove.stoi])