Use Python for this implementation:
Given a csv file "metadata-labeled-conversations.txt" with columns 
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
reason_level_2
plain_whisper
language_code

and a dictionary of phrases and sentences:
{ "call_handling" : ["got wrong information from ... agent",
                     "amount quoted by agent ... incorrect",
					 "was not called back",
					 "no one followed up",
					 "you ... did not follow up",
					 "you ... didn't follow up",
					 "agent ... rude",
					 "you are ... rude",
					 "I was ... cut off",
					 "agent ... tell me ... going to put me on hold"
					 ],
  "payments_transactions" : ["stop payment ... wrong amount",
							 "they said they ... stop the payment",
							 "deposit should ... be more",
							 "deposit should ... been more",
							 "deposit ... for ... different account",
							 "deposit is missing"
							],
  "card" : [ "card not arrived",
			 "card ... not arrive",
			 "card doesn't work",
			 "card does not work",
			 "didn't receive card",
			 "did not receive ... card",
			 "cannot see ... CVV",
			 "chip ... damaged"
			],
  "claims" : ["I already filed ... why ... again",
			  "I never reported ... fraud",
			  "I never applied for ... card",
			  "I've never applied for ... card"
			 ],
  "transfers_disconnects" : ["agent hung up ... me",
							 "you kept transferring me",
							 "you ... routed me ... wrong",
							 "why am I talking ... you"
			 ]
}

The task is to search in the "plain_whisper" column for phrases similar to the phrases of each key in the dictionary.
The ellipsis (the 3 dots ...) in each phrase in the vocabulary means up to 3 words. Therefore any piece of text in the plain_whisper column will be considered a match if it matches a phrase in the vocabulary after the ellipsis (if present) has been taken into account. For example:
"I never applied for zero APR credit card"
would match "I never applied for ... card" since up to 3 tokens (eg. zero APR credit) can be substituted in the dictionary phrase and have it match the phrase in the plain_whisper column
To improve performance, all comparisons should be done in lower case.

If a match is found, then we want to know the corresponding values of conv_id, reason_level_1, complaint_disat, dictionary_key, matching_sentence

where matching_sentence is the entire sentence that the matching fragment of text was found.

All matches should then be written to a csv file, using the columns: conv_id, reason_level_1, complaint_disat, dictionary_key, matching_sentence

--------
import pandas as pd
import re
import csv

# your dictionary
dict_phrases = {
    "call_handling": ["got wrong information from ... agent", "amount quoted by agent ... incorrect", "was not called back",
                      "no one followed up", "you ... did not follow up", "you ... didn't follow up", "agent ... rude",
                      "you are ... rude", "I was ... cut off", "agent ... tell me ... going to put me on hold"],
    "payments_transactions": ["stop payment ... wrong amount", "they said they ... stop the payment",
                              "deposit should ... be more", "deposit should ... been more",
                              "deposit ... for ... different account", "deposit is missing"],
    "card": ["card not arrived", "card ... not arrive", "card doesn't work", "card does not work",
             "didn't receive card", "did not receive ... card", "cannot see ... CVV", "chip ... damaged"],
    "claims": ["I already filed ... why ... again", "I never reported ... fraud", "I never applied for ... card",
               "I've never applied for ... card"],
    "transfers_disconnects": ["agent hung up ... me", "you kept transferring me", "you ... routed me ... wrong",
                              "why am I talking ... you"]
}

# function to loop over rows in the dataframe
def search_phrases_in_dataframe(df, num_words):
    with open('output.csv', 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['conv_id', 'reason_level_1', 'complaint_disat', 'dictionary_key', 'matching_sentence'])
        # loop over rows in the dataframe
        for _, row in df.iterrows():
            # loop over dictionary keys
            for key in dict_phrases.keys():
                # loop over phrases in each key
                for phrase in dict_phrases[key]:
                    # handle ellipsis - replace with regex that matches up to num_words words
                    phrase = re.sub(r'\.\.\.', r'(\\b\\w{1,}\\b ){0,'+str(num_words)+'}', phrase)
                    # split 'plain_whisper' into sentences
                    sentences = str(row['plain_whisper']).lower().split('.')
                    # check each sentence if phrase matches part of it
                    for sentence in sentences:
                        if re.search(phrase, sentence):
                            # if match, write the row data to csv
                            writer.writerow([row['conv_id'], row['reason_level_1'], row['complaint_disat'], key, sentence.strip()])

df = pd.read_csv('metadata-labeled-conversations.txt', sep='\t')

# call function with num_words as 3
search_phrases_in_dataframe(df, 3)

===========================================================
Pull training and testing data from labeled metadata
filename is /chunk-complaint/category/metadata-labeled-conversations.txt with columns:
conv_id
complaint_disat
segment_id
call_duration
reason_level_1
plain_whisper

Given a dataframe of rows with the columns as above, return 2 dataframes. testDataframe will contain 20 percent of rows for each unique type of reason_level_1 rows. For example if there are 4 types of 'reason_level_1' columns (eg. rl1 ... rl4), then the first dataframe will contain 20% of rl1 rows, 20% of rl2 rows, etc. The second dataframw will contain the remaining 80% of each 'row_leval_1 column. The purpose of this split is to create testing and training data. Therefore, the 20% split will be named "testDataFrame" and the 80% split will be named trainingDataFrame". 

